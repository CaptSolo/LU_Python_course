{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LU Logo](https://www.lu.lv/fileadmin/user_upload/LU.LV/www.lu.lv/Logo/Logo_jaunie/LU_logo_LV_horiz.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tīmekļa lapu izstrāde ar Flask un tīmekļa datu iegūšana (rasmošana) ar Scrapy\n",
    "\n",
    "## Nodarbības saturs\n",
    "\n",
    "Šajā nodarbībā mēs apskatīsim šādas tēmas:\n",
    "\n",
    "* Flask - Python servera puses tīmekļa izstrādes rīks jeb ietvars\n",
    "* Scrapy - Python bibliotēka datu iegūšanai no tīmekļa lapām (rasmošanai)\n",
    "\n",
    "## Nodarbības mērķi\n",
    "\n",
    "Nodarbības beigās jūs būsiet spējīgi:\n",
    "\n",
    "* Saprast servera puses tīmekļa izstrādes pamatus\n",
    "* Izveidot vienkāršu tīmekļa lietotni, izmantojot Flask\n",
    "* Saprast tīmekļa lapu rasmošanas pamatus\n",
    "* Iegūt datus no tīmekļa lapām, izmantojot Scrapy\n",
    "\n",
    "## Nepieciešamās priekšzināšanas\n",
    "\n",
    "Pirms sākat šo nodarbību, jums vajadzētu:\n",
    "\n",
    "* Saprast Python programmēšanas pamatus - mainīgie, datu tipi, kontroles struktūras, funkcijas, OOP un failu ievade/izvade\n",
    "* Zināt, kā instalēt Python pakotnes, izmantojot `pip`\n",
    "* Zināt, kā izveidot virtuālo vidi, izmantojot `venv`\n",
    "* Saprast HTML pamatus - skatiet MDN Web Docs [HTML ievads](https://developer.mozilla.org/en-US/docs/Learn/HTML/Introduction_to_HTML)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tīmekļa lapu izstrādes pamati\n",
    "\n",
    "Tīmekļa lapu izstrāde ir plašs jēdziens, kas ietver daudzas dažādas tehnoloģijas un prasmes.\n",
    "\n",
    "Viens dalījums būtu pēc tā, vai jūs strādājat ar lietotāja saskarni (front end) vai ar servera pusi (back end).\n",
    "\n",
    "\n",
    "### Lietotāja saskarne - Front End Web Development\n",
    "\n",
    "Lietotāja saskarnes izstrāde ietver tīmekļa lapas lietotāja saskarnes un lietotāja pieredzes (UX) izveidi. Tas ietver tīmekļa lapas izkārtojuma, krāsu, fontu un interaktīvo elementu projektēšanu. Front end izstrādātāji izmanto HTML, CSS un JavaScript, lai izveidotu tīmekļa lapas vizuālos elementus, ar kuriem lietotāji mijiedarbojas.\n",
    "\n",
    "\n",
    "### Servera izstrāde - Back End Web Development\n",
    "\n",
    "Servera puses tīmekļa izstrāde ietver servera puses loģikas un datu bāzes mijiedarbības izveidi. Tas ietver lietotāja pieprasījumu apstrādi, datu apstrādi un dinamisku satura ģenerēšanu. Servera puses izstrādātāji izmanto servera puses programmēšanas valodas, piemēram, Python, PHP, Ruby, Java un citas, lai izveidotu tīmekļa lapas servera puses komponentus.\n",
    "\n",
    "API izstrāde, lietotāja autentifikācijas apstrāde un datu bāzu pārvaldība ir daži no uzdevumiem, par kuriem atbild back end izstrādātāji.\n",
    "\n",
    "### Pilna tīmekļa izstrāde - Full Stack Web Development\n",
    "\n",
    "Pilna tīmekļa izstrāde ietver darbu gan ar front end, gan ar back end tehnoloģijām. Pilna tīmekļa izstrādātāji ir prasmīgi gan front end, gan back end tehnoloģijās un var izveidot pilnīgas tīmekļa lietotnes no sākuma līdz beigām.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask - Python tīmekļa izstrādes rīks\n",
    "\n",
    "Flask ir vienkāršs un viegli lietojams Python tīmekļa izstrādes rīks. Tas ir paredzēts, lai palīdzētu jums ātri un vienkārši izveidot tīmekļa lietotnes. To izmanto gan pieredzējuši izstrādātāji, gan iesācēji, jo tas ir viegli saprotams un lietojams.\n",
    "\n",
    "### Kā darbojas Flask\n",
    "\n",
    "Flask darbojas, izmantojot WSGI (Web Server Gateway Interface) standartu, kas ļauj tam darboties ar dažādiem tīmekļa serveriem. Tas nozīmē, ka jūs varat izmantot Flask ar dažādiem tīmekļa serveriem, piemēram, Apache, Nginx vai citiem.\n",
    "\n",
    "Flask izmanto dekoratorus, lai definētu maršrutus un funkcijas, kas tiek izpildītas, kad tiek saņemts pieprasījums uz konkrēto maršrutu. Tas ļauj jums viegli definēt, kāda darbība jāveic, kad tiek saņemts pieprasījums uz konkrēto URL. Tos apskatīsim vēlāk šajā nodarbībā."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtuālās vides iestatīšana\n",
    "\n",
    "Pirms instalējat Flask, **izstrādātājiem ir ļoti iesakāms** izveidot savam projektam virtuālo vidi. Tas palīdzēs jums pārvaldīt Python bibliotēku atkarības un izvairīties no konfliktiem ar citiem projektiem.\n",
    "\n",
    "Pastāv vairāki veidi, kā izveidot virtuālo vidi Python. Viena no visbiežāk izmantotajām metodēm ir izmantot iebūvēto `venv` moduli. Šādi varat izveidot virtuālo vidi, izmantojot `venv`:\n",
    "\n",
    "```bash\n",
    "# Create a new directory for your project\n",
    "mkdir myproject\n",
    "cd myproject\n",
    "python -m venv myvenv\n",
    "```\n",
    "\n",
    "Nosaukuma `myenv` vietā jūs varat izmantot jebkuru citu vēlamo nosaukumu savai virtuālajai videi. \n",
    "\n",
    "Vēl viens populārs rīks, kas ļauj izveidot virtuālās vides, ir `uv`: https://docs.astral.sh/uv/\n",
    "\n",
    "#### Virtuālās vides aktivizēšana\n",
    "\n",
    "Lai aktivizētu virtuālo vidi, varat izmantot šādu komandu:\n",
    "\n",
    "```bash\n",
    "# On Windows\n",
    "myvenv\\Scripts\\activate\n",
    "\n",
    "# On macOS and Linux\n",
    "source myvenv/bin/activate\n",
    "```\n",
    "\n",
    "### Flask uzstādīšana\n",
    "\n",
    "Kad esat izveidojis un **aktivizējis** savu virtuālo vidi, varat instalēt Flask, izmantojot `pip`:\n",
    "\n",
    "```bash\n",
    "pip install Flask\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veidojam vienkāršu tīmekļa lietotni ar Flask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Hello, World\" piemērs\n",
    "\n",
    "Ikdienā jūs neveidosiet tīmekļa lietotnes Jupyter notebook vidē, bet uzskatāmības dēļ tomēr pamēģināsim to izdarīt Jupyter vidē:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# izveidosim vienkāršu Flask aplikāciju\n",
    "\n",
    "from flask import Flask # import the Flask class from the flask module\n",
    "\n",
    "app = Flask(__name__) # this creates a new Flask app object\n",
    "\n",
    "# we will be using app.route() decorator to define the URL that will trigger the function below\n",
    "@app.route('/') # this route means that the function below will be called when the user goes to the root URL of your website\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "# you'd add this line to run the app in script mode\n",
    "# if __name__ == '__main__':\n",
    "#     app.run()\n",
    "\n",
    "app.run() # this is the same as the above line, but it's not recommended to use this in script mode\n",
    "# usually you would not run this from Jupiter notebook, but from a terminal\n",
    "\n",
    "# use Ctrl+C to stop the server on terminal\n",
    "# in Jupyter notebook, you can stop the server by clicking on the stop button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametru izmantošana maršrutos\n",
    "\n",
    "Nākošais solis ir izveidot vienkāršu tīmekļa lietotni, kas ņem parametru URL un parāda to lapā. Šeit ir piemērs:\n",
    "\n",
    "\n",
    "```python\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/greet/<name>')\n",
    "def greet(name):\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "# note that the URL is case-sensitive\n",
    "# note the use of <name> in the URL this is a variable part of the URL\n",
    "@app.route('/greet/<name>')\n",
    "def greet(name):\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "# note that only first level of the URL will be caught by this route\n",
    "# so /greet/Janis/Berzins will not work - you would need a separate route for that\n",
    "# but /greet/Janis will work\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n",
    "# use Ctrl+C to stop the server on terminal\n",
    "# in Jupyter notebook, you can stop the server by clicking on the stop button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pieprasījumu parametru izmantošana\n",
    "\n",
    "Pieprasījuma parametri ir vēl viens veids, kā padot datus tīmekļa lietotnei. Tos pievieno URL pēc jautājuma zīmes `?` un tie ir formā `key=value`. Šeit ir piemērs:\n",
    "\n",
    "```python\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/sveiks')\n",
    "def greet():\n",
    "    name = request.args.get('name', 'World')\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "```\n",
    "\n",
    "Tagad jūs varat piekļūt `sveiks` maršrutam ar pieprasījuma parametru šādā formā: `http://localhost:5000/sveiks?name=Uldis`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using query parameters\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/sveiks')\n",
    "def greet():\n",
    "    name = request.args.get('name', 'pasaule') #if no name argument is given, we will use 'Pasaule'\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n",
    "# on local server try something like http://127.0.0.1:5000/sveiks?name=Valdis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Šablonu un statisko failu izmantošana\n",
    "\n",
    "Parasti mēs nevēlamies apstrādāt HTML savā Python kodā. Flask ļauj izmantot šablonus, lai atdalītu HTML no Python koda. Flask izmanto `Jinja2` kā savu šablonu dzinēju.\n",
    "\n",
    "Pilna dokumentācija par Jinja2 atrodama [šeit](https://jinja.palletsprojects.com/en/3.0.x/)\n",
    "\n",
    "Pamatideja ir tāda, ka projektam izveidojat mapi `templates` un ievietojat HTML failus tur. Šabloni ļaus mums ievietot mainīgos un izteiksmes, kas tiks aizstātas ar reāliem datiem, kad šablons tiks apstrādāts (renderēts).\n",
    "\n",
    "Papildus dinamiskam saturam, jums var būt nepieciešami arī statiskie faili, piemēram, CSS, JavaScript, attēli utt. Lai tos iekļautu savā projektā, izveidojiet mapi `static` un ievietojiet šos failus tur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using templates and static files\n",
    "from flask import Flask, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    items = [\"Saraksti\", \"Vārdnīcas\", \"Citi objekti\"] # these could be from a database or other data source\n",
    "    # also these objects could be from url query parameters\n",
    "    return render_template('index.html', year=2024, items=items) # thus template will receive year and items\n",
    "    \n",
    "# see index.html templates folder on how it is handled on the template side\n",
    "# you might also check out base.html to see how templates can be extended\n",
    "\n",
    "@app.route('/about')\n",
    "def about():\n",
    "    return render_template('about.html', year=2024)\n",
    "    \n",
    "# about.html is even simpler template than index.html\n",
    "# it also extends base.html in templates folder\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # app.run(debug=True) # debug mode will reload the server on code changes and provide more verbose output\n",
    "    app.run() # debug mode will reload the server on code changes and provide more verbose output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask projektu struktūra\n",
    "\n",
    "Lai jūsu Flask projekts būtu labi organizēts, jūs varat izmantot šādu struktūru:\n",
    "\n",
    "```\n",
    "project_root/\n",
    "│\n",
    "├── app.py                # Jūsu galvenais Flask fails - iespējams, ka būs arī citi .py faili\n",
    "├── static/               # Mape statiskiem failiem (CSS, JavaScript, attēli, u.t.t.)\n",
    "│   └── styles.css        # Jūsu CSS fails - var, protams, būt vairāki\n",
    "|   └── script.js         # Jūsu JavaScript fails - var, protams, būt vairāki\n",
    "├── templates/            # Mape šabloniem\n",
    "│   └── base.html         # Bāzes šablons - var, protams, būt vairāki\n",
    "│   └── index.html        # Citi šabloni - var, protams, būt vairāki\n",
    "└── requirements.txt      # (Ieteicams, bet ne obligāts) saraksts ar visām nepieciešamajām bibliotēkām\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mācību materiāli un resursi\tFlask apguvei\n",
    "\n",
    "*Lai būtu pilnvērtīgs Flask izstrādātājs, jums būs nepieciešams iemācīties vairāk par šādām tēmām:*\t\n",
    "\n",
    "- Formu apstrāde ar Flask\n",
    "- Datu bāzu izmantošana ar Flask, piemēram, ar SQLAlchemy vai citiem ORM (Object-Relational Mapping) rīkiem\n",
    "- Lietotāju autentifikācija ar Flask\n",
    "- Sesiju pārvaldība ar Flask\n",
    "- Flask lietotņu izvietošana - AWS, PythonAnywhere, DigitalOcean, lokālais serveris u.t.t.\n",
    "\n",
    "Lai turpinātu apgūt Flask, jūs varat izmantot šādus resursus:\n",
    "- Oficiālā Flask dokumentācija: https://flask.palletsprojects.com/en/2.0.x/\n",
    "- Miguel Grinberg's Flask Mega-Tutorial: https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world\n",
    "- Corey Schafer's Flask Tutorial Series: https://www.youtube.com/playlist?list=PL-osiE80TeTs4UjLw5MM6OjgkjFeUxCYH\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tīmekļa lapu rasmošana - skrāpēšana - kas tas ir?\n",
    "\n",
    "Tīmekļa lapu rasmošana ir process datu iegūšanai no tīmekļa lapām. Tas ietver HTTP pieprasījumu nosūtīšanu uz tīmekļa lapu, HTML satura apstrādi (parsēšanu) un nepieciešamo datu iegūšanu. Tīmekļa lapu rasmošana tiek izmantota datu iegūšanai, cenu uzraudzībai, satura apkopošanai un citiem mērķiem.\n",
    "\n",
    "Teorētiski iespējams rasmošanu veikt arī manuāli, vienkārši saglabājot apmeklētās tīmekļa lapas saturu, bet bieži vien ir efektīvāk izmantot tīmekļa rasmošanas bibliotēku, piemēram, Scrapy, lai automatizētu šo procesu.\n",
    "\n",
    "### Rasmošanas noteikumi\n",
    "\n",
    "* Spēlējiet tīri! Neveidojiet pārlieku slodzi tīmekļa vietnei ar pieprasījumiem, jo tas var izraisīt servera problēmas un jūs varat tikt nobloķēts.\n",
    "* Pirms sākat rasmošanu, pārbaudiet tīmekļa vietnes lietošanas noteikumus un robots.txt failu, lai pārliecinātos, ka nepārkāpjat kādus noteikumus.\n",
    "* Ja iespējams iegūt datus caur API, ir ieteicams izmantot API, nevis rasmošanu.\n",
    "* Iegūtos datus izmantojiet tikai saskaņā ar autortiesībām un pastāvošajiem likumiem.\n",
    "* Ja iespējams, izmantojiet jau publisku datu kopu - vislabāk no pašiem lietotnes autoriem - nevis tīmekļa lapas rasmošanu.\n",
    "* Izmantojiet tīmekļa lapu rasmošanu atbildīgi un ētiski.\n",
    "* Ievērojiet atšķirību starp kādu datu rasmošanu un to publicēšanu. Rasmošana ir tikai datu iegūšana, bet publicēšana ir atsevišķs jautājums.\n",
    "* Pētnieciskiem mērķiem ir ieteicams iegūt atļauju no tīmekļa vietnes īpašniekiem, ja tas iespējams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy - Python tīmekļa rasmošanas bibliotēka\n",
    "\n",
    "[Scrapy](https://scrapy.org) ir jaudīga Python tīmekļa rasmošanas bibliotēka, kas atvieglo datu iegūšanu no tīmekļa vietnēm. Tā nodrošina augsta līmeņa API tīmekļa vietņu pārlūkošanai un datu iegūšanai, tādēļ tā ir lieliska izvēle tīmekļa rasmošanas projektos.\n",
    "\n",
    "### Scrapy alternatīva - BeautifulSoup\n",
    "\n",
    "Beautiful Soup ir vēl viena populāra Python tīmekļa rasmošanas bibliotēka. Tā ir \"viegla\" bibliotēka, kas ir vienkārši lietojama un lieliska vienkāršiem tīmekļa rasmošanas uzdevumiem. Tomēr, ja jums ir nepieciešamas papildu funkcijas, piemēram, vairāku lapu pārlūkošana, sīkdatņu un sesiju apstrāde, un datu iegūšana no sarežģītām tīmekļa vietnēm, Scrapy ir labāka izvēle.\n",
    "\n",
    "Vairāk par BeautifulSoup varat uzzināt šeit: https://www.crummy.com/software/BeautifulSoup/\n",
    "\n",
    "### Kā strādā Scrapy\n",
    "\n",
    "Scrapy strādā, nosūtot HTTP pieprasījumus uz tīmekļa vietni, parsējot HTML saturu un izmantojot selektoru sistēmu, lai iegūtu nepieciešamos datus. Tas nodrošina vairākas funkcijas, kas padara tīmekļa rasmošanu vieglu un efektīvu, piemēram, iebūvēto tīmekļa pārlūkošanu, jaudīgu selektoru sistēmu un atbalstu sīkdatnēm un sesijām.\n",
    "\n",
    "### Scrapy uzstādīšana\n",
    "\n",
    "Kā parasti, pirms instalējat Scrapy, ieteicams vispirms izveidot un aktivizēt **virtuālo vidi**. Skat. iepriekšējās Flask uzstādīšanas instrukcijas.\n",
    "\n",
    "Scrapy var instalēt ar `pip`:\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install scrapy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vienkāršs tīmekļa rasmošanas piemērs ar Scrapy\t\n",
    "\n",
    "Pieņemsim, ka mēs vēlamies iegūt informāciju par pilsētām un to iedzīvotāju skaitu no Vikipēdijas lapas: https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\n",
    "\n",
    "(Lūdzu ņemiet vērā, ka Vikipēdija piedāvā [API](https://www.mediawiki.org/wiki/API:Main_page), lai piekļūtu saviem datiem, tāpēc rasmošana šajā gadījumā nav nepieciešama. Tas ir tikai mācību piemērs.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy version is 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# first let's import scrapy and check its version\n",
    "try:\n",
    "    import scrapy\n",
    "    print(f\"Scrapy version is {scrapy.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Scrapy is not installed\")\n",
    "    print(\"You can install Scrapy with pip install scrapy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a good practice (and is required by Wikipedia) to provide information about your scraper.\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"LU_PythonCourse/1.0 (https://github.com/CaptSolo/LU_Python_course/; captsolo@gmail.com)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will scrape https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\n",
      "200\n",
      "Web page title is: List of cities and towns in Latvia - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "# now let's scrape the following web page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_cities_in_Latvia'\n",
    "print(f\"We will scrape {url}\")\n",
    "\n",
    "# Note: usually there is no need to scrape Wikipedia as they have APIs and data dumps available\n",
    "# but for learning purposes we will scrape this page\n",
    "# we are interested in a table with cities and their population\n",
    "\n",
    "# we've already imported scrapy so we can start using it\n",
    "# let's start with basic example where we simply fetch the page and extract the title\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "try:\n",
    "    import requests  # we also use requests library to fetch the page\n",
    "except ImportError:\n",
    "    print(\"Requests is not installed\")\n",
    "    print(\"You can install Requests with pip install requests\")\n",
    "\n",
    "# NOTE: Scrapy has its own request object as well, but it is more complex\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "# let's create a scrapy response object\n",
    "scrapy_response = HtmlResponse(url, body=response.text, encoding='utf-8')\n",
    "\n",
    "print(response.status_code)\n",
    "\n",
    "# let's extract the title using css selector\n",
    "title = scrapy_response.css('title::text').get()\n",
    "print(\"Web page title is:\", title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS selektoru izmantošana datu iegūšanai\n",
    "\n",
    "Scrapy piedāvā jaudīgu selektoru sistēmu, kas ļauj jums izgūt datus no HTML izmantojot CSS selektorus. Jūs varat izmantot CSS selektorus, lai norādītu uz konkrētiem elementiem lapā un iegūtu nepieciešamos datus.\n",
    "\n",
    "Lai izmantotu CSS selektorus Scrapy, jums būs jāzina CSS selektoru sintakse. Šeit ir daži pamata piemēri:\n",
    "\n",
    "#### Pamata CSS piemēri:\n",
    "\n",
    "| CSS selektors | Apraksts |\n",
    "| --- | --- |\n",
    "| tag | Izvēlas visus <tag> elementus. |\n",
    "| .class | Izvēlas visus elementus ar klasi class. |\n",
    "| #id | Izvēlas elementu ar ID id. |\n",
    "| tag.class | Izvēlas <tag> elementus ar klasi class. |\n",
    "| tag[attr=\"value\"] | Izvēlas <tag> elementus ar atribūtu attr=\"value\". |\n",
    "| tag > child | Izvēlas <tag> elementa tiešos bērnus. |\n",
    "| tag child | Izvēlas visus <tag> elementa pēcnācējus. |\n",
    "| tag:first-child | Izvēlas pirmo <tag> bērnu. |\n",
    "| tag:nth-child(n) | Izvēlas n-to <tag> bērnu. |\n",
    "\n",
    "#### Scrapy CSS metodes: \n",
    "\n",
    "- `response.css('<CSS selector>')`: Izgūst elementus, kas atbilst CSS selektoram.\n",
    "- `.get()`: Atgriež pirmo atbilstošo vērtību.\n",
    "- `.getall()`: Atgriež visas atbilstošās vērtības sarakstā.\n",
    "- `.re('<regex>')`: Izgūst vērtības, kas atbilst regulārajai izteiksmei.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 tables on the page\n",
      "There are 2 wikitable sortable tables on the page\n",
      "Table(s) have rows 83\n"
     ]
    }
   ],
   "source": [
    "# now let's use Selectors to get the table with cities and their population\n",
    "# we will use CSS selectors\n",
    "# we will use the table with classes wikitable sortable\n",
    "\n",
    "# first let's see about getting all tables\n",
    "tables = scrapy_response.css('table')\n",
    "\n",
    "# how many tables are there?\n",
    "print(\"There are\", len(tables), \"tables on the page\")\n",
    "\n",
    "# by using inspect element in browser we can see that the table we want is the first table with class wikitable sortable\n",
    "table = scrapy_response.css('table.wikitable.sortable') \n",
    "\n",
    "# how many tables are there?\n",
    "print(\"There are\", len(table), \"wikitable sortable tables on the page\")\n",
    "\n",
    "# in our case this is fine as we want elements from both tables\n",
    "# if we only needed the first table we could use table = scrapy_response.css('table.wikitable.sortable')[0] as the first table is at index 0\n",
    "\n",
    "# how many tr elements are in the tables\n",
    "rows = table.css('tr')\n",
    "print(\"Table(s) have rows\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest five cities [{'city': ['Rīga', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '658,640\\n'}, {'city': ['Daugavpils', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '93,312\\n'}, {'city': ['Liepāja', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '76,731\\n'}, {'city': ['Jelgava', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '59,511\\n'}, {'city': ['Jūrmala', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '50,840\\n'}]\n",
      "Smallest five cities [{'city': ['Varakļāni', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '2,106\\n'}, {'city': ['Viesīte', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '1,902\\n'}, {'city': ['Viļaka', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '1,525\\n'}, {'city': ['Viļāni', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '3,468\\n'}, {'city': ['Zilupe', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '1,746\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# let's save the data in a list of dictionaries for easier processing\n",
    "cities = []\n",
    "\n",
    "# let's iterate over rows and extract the data\n",
    "for row in rows:\n",
    "    # city = row.css('td:nth-child(1)::text').get()\n",
    "    # we want all text from first child td element that is contained in its children\n",
    "    city = row.css('td:nth-child(1) *::text').getall() # note the * after td:nth-child(1) we get all text from all children\n",
    "    population = row.css('td:nth-child(2)::text').get() # here we get only the text from the td element\n",
    "    if city and population: # we only want rows with city and population\n",
    "        cities.append({'city': city, 'population': population})\n",
    "        \n",
    "# print first 5 cities\n",
    "print(\"Biggest five cities\", cities[:5])\n",
    "\n",
    "# last 5 cities\n",
    "print(\"Smallest five cities\", cities[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'city': ['Rīga', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'],\n",
      "  'population': '658,640\\n'},\n",
      " {'city': ['Daugavpils', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'],\n",
      "  'population': '93,312\\n'},\n",
      " {'city': ['Liepāja', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'],\n",
      "  'population': '76,731\\n'},\n",
      " {'city': ['Jelgava', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'],\n",
      "  'population': '59,511\\n'},\n",
      " {'city': ['Jūrmala', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'],\n",
      "  'population': '50,840\\n'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# \"pretty-print\" the first 5 cities\n",
    "pprint(cities[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest five cities [{'city': 'Rīga', 'population': '658,640\\n'}, {'city': 'Daugavpils', 'population': '93,312\\n'}, {'city': 'Liepāja', 'population': '76,731\\n'}, {'city': 'Jelgava', 'population': '59,511\\n'}, {'city': 'Jūrmala', 'population': '50,840\\n'}]\n",
      "Smallest five cities [{'city': 'Varakļāni', 'population': '2,106\\n'}, {'city': 'Viesīte', 'population': '1,902\\n'}, {'city': 'Viļaka', 'population': '1,525\\n'}, {'city': 'Viļāni', 'population': '3,468\\n'}, {'city': 'Zilupe', 'population': '1,746\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# we can see that we only care about the first entry for city name so let's clean up the list of dictionaries\n",
    "# we will keep only the first entry for city name\n",
    "cities = [{'city': d['city'][0], 'population': d['population']} for d in cities] # list comprehension\n",
    "# print first 5 cities\n",
    "print(\"Biggest five cities\", cities[:5])\n",
    "# last 5 cities\n",
    "print(\"Smallest five cities\", cities[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'city': 'Rīga', 'population': '658,640\\n'},\n",
      " {'city': 'Daugavpils', 'population': '93,312\\n'},\n",
      " {'city': 'Liepāja', 'population': '76,731\\n'},\n",
      " {'city': 'Jelgava', 'population': '59,511\\n'},\n",
      " {'city': 'Jūrmala', 'population': '50,840\\n'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(cities[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we could save the results to a file or database\n",
    "# alternatively we could load the data into a pandas DataFrame for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XPath selektoru izmantošana datu iegūšanai\n",
    "\n",
    "XPath ir jaudīgāka alternatīva CSS selektoriem, kas ļauj jums veikt sarežģītākus datu iegūšanas uzdevumus. XPath ļauj jums navigēt un izvēlēties elementus HTML vai XML dokumentā. Scrapy `response.xpath()` metode nodrošina saskarni elementu atlasīšanai, kas izmanto XPath izteiksmes.\n",
    "\n",
    "Lai izmantotu XPath iekš Scrapy, jums būs jāzina XPath sintakse. Šeit ir daži pamata piemēri:\n",
    "\n",
    "#### Pamata XPath piemēri:\n",
    "\n",
    "| XPath izteiksmes | Apraksts |\n",
    "| --- | --- |\n",
    "| //tag | Izvēlas visus <tag> elementus jebkur, kur tie atrodas dokumentā. |\n",
    "| ./tag | Izvēlas visus <tag> elementus tieši zem pašreizējā elementa. |\n",
    "| //tag[@attr=\"value\"] | Izvēlas <tag> elementus ar atribūtu attr=\"value\". |\n",
    "| //tag/text() | Izvēlas <tag> elementu teksta saturu. |\n",
    "| //tag[contains(@attr, \"val\")] | Izvēlas <tag> elementus, kur attr satur \"val\". |\n",
    "| //tag[1] | Izvēlas pirmo <tag> elementu kontekstā. |\n",
    "| //tag[last()] | Izvēlas pēdējo <tag> elementu kontekstā. |\n",
    "| //tag[position() < 3] | Izvēlas pirmos divus <tag> elementus. |\n",
    "\n",
    "#### Scrapy XPath metodes:\n",
    "\n",
    "- `response.xpath('<XPath>')`: Izgūst elementus, kas atbilst XPath izteiksmēm.\n",
    "- `.get()`: Atgriež pirmo atbilstošo vērtību.\n",
    "- `.getall()`: Atgriež visas atbilstošās vērtības sarakstā.\n",
    "- `.extract()`: Sinonīms `.getall()`, bet ir novecojis.\n",
    "- `.re('<regex>')`: Izgūst vērtības, kas atbilst regulārai izteiksmē.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 wikitable sortable tables on the page\n"
     ]
    }
   ],
   "source": [
    "# now let's see how we could have used XPath selectors to get the same data\n",
    "\n",
    "# let's extract the table with XPath\n",
    "table = scrapy_response.xpath('//table[contains(@class, \"wikitable\") and contains(@class, \"sortable\")]')\n",
    "\n",
    "# how many tables are there?\n",
    "print(\"There are\", len(table), \"wikitable sortable tables on the page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table(s) have rows 83\n"
     ]
    }
   ],
   "source": [
    "# now let's extract the rows with XPath\n",
    "rows = table.xpath('.//tr')\n",
    "print(\"Table(s) have rows\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try something slightly fancier - we will want to extract text from the first anchor child in the first td element - city name\n",
    "\n",
    "# example\n",
    "# <tr>\n",
    "# <td><a href=\"/wiki/R%C4%ABga\" class=\"mw-redirect\" title=\"Rīga\">Rīga</a>&nbsp;<small><span class=\"noprint\"><span class=\"ext-phonos\"><span data-nosnippet=\"\" id=\"ooui-php-1\" class=\"ext-phonos-PhonosButton noexcerpt oo-ui-widget oo-ui-widget-enabled oo-ui-buttonElement oo-ui-buttonElement-frameless oo-ui-iconElement oo-ui-labelElement oo-ui-buttonWidget\" data-ooui=\"{&quot;_&quot;:&quot;mw.Phonos.PhonosButton&quot;,&quot;href&quot;:&quot;\\/\\/upload.wikimedia.org\\/wikipedia\\/commons\\/transcoded\\/f\\/f5\\/Lv-R%C4%ABga.ogg\\/Lv-R%C4%ABga.ogg.mp3&quot;,&quot;rel&quot;:[&quot;nofollow&quot;],&quot;framed&quot;:false,&quot;icon&quot;:&quot;volumeUp&quot;,&quot;label&quot;:{&quot;html&quot;:&quot;pronunciation&quot;},&quot;data&quot;:{&quot;ipa&quot;:&quot;&quot;,&quot;text&quot;:&quot;&quot;,&quot;lang&quot;:&quot;en&quot;,&quot;wikibase&quot;:&quot;&quot;,&quot;file&quot;:&quot;Lv-R\\u012bga.ogg&quot;},&quot;classes&quot;:[&quot;ext-phonos-PhonosButton&quot;,&quot;noexcerpt&quot;]}\"><a role=\"button\" tabindex=\"0\" href=\"//upload.wikimedia.org/wikipedia/commons/transcoded/f/f5/Lv-R%C4%ABga.ogg/Lv-R%C4%ABga.ogg.mp3\" rel=\"nofollow\" aria-label=\"Play audio\" title=\"Play audio\" class=\"oo-ui-buttonElement-button\"><span class=\"oo-ui-iconElement-icon oo-ui-icon-volumeUp\"></span><span class=\"oo-ui-labelElement-label\">pronunciation</span><span class=\"oo-ui-indicatorElement-indicator oo-ui-indicatorElement-noIndicator\"></span></a></span><sup class=\"ext-phonos-attribution noexcerpt navigation-not-searchable\"><a href=\"/wiki/File:Lv-R%C4%ABga.ogg\" title=\"File:Lv-Rīga.ogg\">ⓘ</a></sup></span></span></small>&nbsp;<figure class=\"mw-halign-right\" typeof=\"mw:File\"><a href=\"/wiki/File:Greater_Coat_of_Arms_of_Riga_-_for_display.svg\" class=\"mw-file-description\"><img src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Greater_Coat_of_Arms_of_Riga_-_for_display.svg/55px-Greater_Coat_of_Arms_of_Riga_-_for_display.svg.png\" decoding=\"async\" width=\"55\" height=\"33\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Greater_Coat_of_Arms_of_Riga_-_for_display.svg/83px-Greater_Coat_of_Arms_of_Riga_-_for_display.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/99/Greater_Coat_of_Arms_of_Riga_-_for_display.svg/110px-Greater_Coat_of_Arms_of_Riga_-_for_display.svg.png 2x\" data-file-width=\"510\" data-file-height=\"303\"></a><figcaption></figcaption></figure>\n",
    "# </td>\n",
    "# <td>658,640\n",
    "# </td>\n",
    "# <td>632,614\n",
    "# </td>\n",
    "# <td>605,273\n",
    "# </td></tr>\n",
    "\n",
    "# here we can see that city name Rīga is in the first td element inside its first anchor child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest five cities\n",
      "[{'city': 'Rīga', 'population': '658,640\\n'},\n",
      " {'city': 'Daugavpils', 'population': '93,312\\n'},\n",
      " {'city': 'Liepāja', 'population': '76,731\\n'},\n",
      " {'city': 'Jelgava', 'population': '59,511\\n'},\n",
      " {'city': 'Jūrmala', 'population': '50,840\\n'}]\n",
      "\n",
      "Smallest five cities\n",
      "[{'city': 'Varakļāni', 'population': '2,106\\n'},\n",
      " {'city': 'Viesīte', 'population': '1,902\\n'},\n",
      " {'city': 'Viļaka', 'population': '1,525\\n'},\n",
      " {'city': 'Viļāni', 'population': '3,468\\n'},\n",
      " {'city': 'Zilupe', 'population': '1,746\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# now let's save data in a list of dictionaries for easier processing\n",
    "also_cities = []\n",
    "\n",
    "# let's iterate over rows and extract the data\n",
    "for row in rows:\n",
    "    # city = row.xpath('.//td[1]//text()').getall() # we want all text from first child td element that is contained in its children\n",
    "    # we want text from first anchor element in td[1]\n",
    "    city = row.xpath('.//td[1]//a[1]//text()').get() # note the //a[1] we get text from first anchor child of the first td element\n",
    "    population = row.xpath('.//td[2]//text()').get() # here we get only the text from the td element\n",
    "    if city and population: # we only want rows with city and population\n",
    "        also_cities.append({'city': city, 'population': population})\n",
    "        \n",
    "# print first 5 cities\n",
    "print(\"Biggest five cities\")\n",
    "pprint(also_cities[:5])\n",
    "\n",
    "# last 5 cities\n",
    "print(\"\\nSmallest five cities\")\n",
    "pprint(also_cities[-5:])\n",
    "\n",
    "# using XPath we did not have to do any Python list comprehension to clean up the data after extraction\n",
    "# now we could still clean up newlines and convert population to integer but that is easy and not part of scraping itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atšķirības starp CSS un XPath pieprasījumiem Scrapy\n",
    "\n",
    "| Īpašība | XPath | CSS |\n",
    "| --- | --- | --- |\n",
    "| Sintakse | Izteiksmīgāka un sarežģīta | Vienkārša un viegli lasāma |\n",
    "| Veiktspēja | Parasti lēnāka | Parasti ātrāka |\n",
    "| Teksta saturs | Jāizmanto `text()` funkcija | Izmanto `::text` selektoru |\n",
    "| Atribūtu atbilstība | Izmanto `@attr=\"value\"` | Izmanto `[attr=\"value\"]` selektoru |\n",
    "| Pozicionālā atbilstība | Izmanto `position()=n` | Izmanto `:nth-child(n)` selektoru |\n",
    "| Navigācija | Atbalsta vecāku/brāļu navigāciju | Ierobežots tikai ar bērnu navigāciju |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vairāku lapu rasmošana ar Scrapy\n",
    "\n",
    "Vairāku lapu rasmošana ir bieži sastopams uzdevums, kad jums ir nepieciešams iegūt datus no vairākām lapām. Scrapy piedāvā iespēju sekot saitēm un iegūt datus no vairākām lapām, izmantojot tajā iebūvēto tīmekļa pārlūkošanas funkciju.\t\n",
    "\n",
    "Pagaidām mēs izmantojam iepriekš noteiktu URL sarakstu, bet reālā dzīvē jūs, iespējams, vēlēsieties rasmot saites no kādas lapas/lapām pašām."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will scrape the following pages:\n",
      "https://en.wikipedia.org/wiki/List_of_cities_in_Estonia\n",
      "https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\n",
      "https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania\n"
     ]
    }
   ],
   "source": [
    "start_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Estonia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania\",\n",
    "]\n",
    "print(\"We will scrape the following pages:\", *start_urls, sep=\"\\n\") # we using * to unpack the list into separate arguments for neat printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 18:33:20 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: scrapybot)\n",
      "2025-12-09 18:33:20 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.11.0, Python 3.12.7 (main, Oct 16 2024, 07:12:08) [Clang 18.1.8 ], pyOpenSSL 24.3.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform macOS-14.7.6-arm64-arm-64bit\n",
      "2025-12-09 18:33:20 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2025-12-09 18:33:20 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2025-12-09 18:33:20 [scrapy.extensions.telnet] INFO: Telnet Password: bcd45747f1a5006f\n",
      "2025-12-09 18:33:20 [py.warnings] WARNING: /Users/captsolo/Documents/Code/LU_Python_course/.venv/lib/python3.12/site-packages/scrapy/extensions/feedexport.py:432: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n",
      "2025-12-09 18:33:20 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2025-12-09 18:33:20 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'USER_AGENT': 'LU_PythonCourse/1.0 '\n",
      "               '(https://github.com/CaptSolo/LU_Python_course/; '\n",
      "               'captsolo@gmail.com)'}\n",
      "2025-12-09 18:33:20 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2025-12-09 18:33:20 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2025-12-09 18:33:20 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2025-12-09 18:33:20 [scrapy.core.engine] INFO: Spider opened\n",
      "2025-12-09 18:33:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-12-09 18:33:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2025-12-09 18:33:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_cities_in_Latvia> (referer: None)\n",
      "2025-12-09 18:33:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania> (referer: None)\n",
      "2025-12-09 18:33:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_cities_in_Estonia> (referer: None)\n",
      "2025-12-09 18:33:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/List_of_cities_in_Latvia>\n",
      "{'title': 'List of cities and towns in Latvia - Wikipedia'}\n",
      "2025-12-09 18:33:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania>\n",
      "{'title': 'List of cities in Lithuania - Wikipedia'}\n",
      "2025-12-09 18:33:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/List_of_cities_in_Estonia>\n",
      "{'title': 'List of cities and towns in Estonia - Wikipedia'}\n",
      "2025-12-09 18:33:20 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2025-12-09 18:33:20 [scrapy.extensions.feedexport] INFO: Stored json feed (3 items) in: cities_titles.json\n",
      "2025-12-09 18:33:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 898,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 122909,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 3,\n",
      " 'elapsed_time_seconds': 0.32912,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2025, 12, 9, 16, 33, 20, 770249, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 813475,\n",
      " 'httpcompression/response_count': 3,\n",
      " 'item_scraped_count': 3,\n",
      " 'items_per_minute': None,\n",
      " 'log_count/DEBUG': 7,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 114688000,\n",
      " 'memusage/startup': 114688000,\n",
      " 'response_received_count': 3,\n",
      " 'responses_per_minute': None,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2025, 12, 9, 16, 33, 20, 441129, tzinfo=datetime.timezone.utc)}\n",
      "2025-12-09 18:33:20 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# we have the start_urls now we can create a Scrapy spider to scrape these pages\n",
    "# in this case we will use a simple spider that will only extract the title of the page\n",
    "\n",
    "# let's do a simple spider that will extract the title of the page\n",
    "\n",
    "# we want to save these titles in a list of dictionaries as JSON\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class SimpleSpider(scrapy.Spider):\n",
    "    \n",
    "    name = 'simple_spider'\n",
    "    \n",
    "    start_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Estonia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania\",\n",
    "] # we could have passed this as a parameter to the spider\n",
    "\n",
    "    custom_settings = {\n",
    "        'USER_AGENT': 'LU_PythonCourse/1.0 (https://github.com/CaptSolo/LU_Python_course/; captsolo@gmail.com)',\n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').get()\n",
    "        # ADD your own code here to extract more data from each page\n",
    "        # you can use either css or xpath selectors\n",
    "        yield {'title': title}\n",
    "\n",
    "# let's run the spider\n",
    "# we also want to save the results to a file\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    'FEED_URI': 'cities_titles.json',\n",
    "    'FEED_FORMAT': 'json'\n",
    "})\n",
    "\n",
    "process.crawl(SimpleSpider)\n",
    "process.start()\n",
    "# NOTE this process is really meant to be run from a script or terminal\n",
    "# if you run it in a Jupyter notebook you might have to restart the kernel to run it again\n",
    "# otherwise you will get errors about already running Twisted reactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open(\"cities_titles.json\") as in_file:\n",
    "    data = json.load(in_file)\n",
    "    \n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vairāku lapu rasmošanas piemērs ar Scrapy\n",
    "\n",
    "1. **Izveidojiet zirnekli - Spider** : \n",
    "   - Iekļaujiet visus URL sarakstā `start_urls` vai ielādējiet tos dinamiski, izmantojot `start_requests()`.\n",
    "\n",
    "2. **Parsējiet katru lapu** :\n",
    "   - Izmantojiet to pašu `parse()` metodi datu izgūšanai, jo lapu struktūra ir līdzīga.\n",
    " \n",
    "3. **Identificējiet kontekstu** :\n",
    "   - Izmantojiet informāciju no URL vai lapas satura, lai iezīmētu datus (piemēram, valsts nosaukumu).\n",
    "\n",
    "4. **Palaidiet zirnekli** :\n",
    "   - Palaidiet zirnekli un, iespējams, saglabājiet tā rezultātus failā. \n",
    "\n",
    "5. **Pēcapstrāde** :\n",
    "   - Pārbaudiet un attīriet datus pēc vajadzības (piem., apvienojiet JSON failus, CSV tabulas).\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy mācību materiāli un resursi\n",
    "\n",
    "Lai turpinātu apgūt Scrapy, jūs varat izmantot šādus resursus:\n",
    "\n",
    "- Oficiālā Scrapy dokumentācija: https://docs.scrapy.org/en/latest/\n",
    "- Scrapy Tutorial: https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    "- YouTube Tutorial from FreeCodeCamp: https://www.youtube.com/watch?v=mBoX_JCKZTE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praktiskie uzdevumi\n",
    "\n",
    "### Flask uzdevums\n",
    "\n",
    "1. Izveidojiet vienkāršu tīmekļa lietotni, izmantojot Flask, kas parāda sarakstu ar kādiem elementiem. Saraksts jāglabā Python sarakstā un jāparāda tīmekļa lapā.\n",
    "2. Izveidojiet formu aplikācijā, kas ļauj lietotājiem pievienot sarakstam jaunus elementus.\n",
    "3. Izveidojiet dzēšanas pogu blakus katram saraksta elementam, kas ļauj lietotājiem dzēst elementus no saraksta.\n",
    "4. Glabājiet datus par saraksta elementiem datu bāzē (ir atļauts izmantot SQLite datu bāzi, bet varat izmantot arī citus datu bāzu risinājumus).\n",
    "\n",
    "### Scrapy uzdevums\n",
    "\n",
    "1. Izveidojiet Scrapy zirnekli, kas rasmo datus no jūsu izvēlētas tīmekļa lapas. Zirneklim ir jāizgūst vismaz divi lauki no tīmekļa lapas un jāsaglabā datus JSON vai CSV failā.\n",
    "2. Modificējiet zirnekli, lai saglabātu datus datu bāzē, nevis JSON vai CSV failā.\n",
    "3. Pievienojiet zirneklim kļūdu apstrādi, lai apstrādātu gadījumus, kad tīmekļa vietne ir nesasniedzama vai visi datu lauki nav pieejami.\n",
    "\n",
    "## Kopsavilkums\n",
    "\n",
    "Šajā nodarbībā mēs apskatījām tīmekļa lapu izstrādes pamatus, izmantojot Flask, un datu iegūšanu no tīmekļa lapām, izmantojot Scrapy. Mēs iemācījāmies izveidot vienkāršu tīmekļa lietotni ar Flask un kā rasmot datus no tīmekļa lapām, izmantojot Scrapy. Mēs arī apspriedām tīmekļa rasmošanas noteikumus un labākos veidus, kā strādāt ar tīmekļa rasmošanas bibliotēkām.\n",
    "\n",
    "Lai turpinātu apgūt zināšanas par tīmekļa izstrādi un tīmekļa datu iegūšanu, jums būs jāizmēģina praksē dažādus uzdevumus un projektus. \n",
    "\n",
    "Veiksmi darbā ar Flask un Scrapy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
