{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LU Logo](https://www.lu.lv/fileadmin/user_upload/LU.LV/www.lu.lv/Logo/Logo_jaunie/LU_logo_LV_horiz.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tīmekļa lapu izstrāde ar Flask un datu iegūšana(rasmošana) ar Scrapy\n",
    "\n",
    "## Nodarbības saturs\n",
    "\n",
    "Mēs apskatīsim šādas tēmas šajā nodarbībā:\n",
    "\n",
    "* Flask - Python servera puses tīmekļa izstrādes rīks jeb sastatnes\n",
    "* Scrapy - Python bibliotēka datu iegūšanai no tīmekļa lapām, jeb rasmošanai\n",
    "\n",
    "## Nodarbības mērķi\n",
    "\n",
    "Nodarbības beigās jūs būsiet spējīgi:\n",
    "\n",
    "* Saprast servera puses tīmekļa izstrādes pamatus\n",
    "* Izveidot vienkāršu tīmekļa lietotni, izmantojot Flask\n",
    "* Saprast tīmekļa lapu rasmošanas pamatus\n",
    "* Iegūt datus no tīmekļa lapas, izmantojot Scrapy\n",
    "\n",
    "## Nepieciešamās priekšzināšanas\n",
    "\n",
    "Pirms sākat šo nodarbību, jums vajadzētu:\n",
    "\n",
    "* Saprast Python programmēšanas pamatus - mainīgie, datu tipi, kontroles struktūras, funkcijas, OOP un failu ievade/izvade\n",
    "* Zināt, kā instalēt Python pakotnes, izmantojot `pip`\n",
    "* Zināt, kā izveidot virtuālo vidi, izmantojot `venv`\n",
    "* Saprast HTML pamatus - skatiet MDN Web Docs [HTML ievads](https://developer.mozilla.org/en-US/docs/Learn/HTML/Introduction_to_HTML)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Front End Web Development vs. Back End Web Development\n",
    "\n",
    "Tīmekļa lapu izstrāde ir plašs jēdziens, kas ietver daudzas dažādas tehnoloģijas un prasmes.\n",
    "\n",
    "Viens dalījums būtu pēc tā, vai jūs strādājat ar lietotāja saskarni (front end) vai ar servera pusi (back end).\n",
    "\n",
    "\n",
    "### Lietotājā saskarne - Front End Web Development\n",
    "\n",
    "Lietotāja saskarnes izstrāde ietver tīmekļa lapas lietotāja saskarnes un lietotāja pieredzes(UX) izveidi. Tas ietver tīmekļa lapas izkārtojuma, krāsu, fontu un interaktīvo elementu projektēšanu. Front end izstrādātāji izmanto HTML, CSS un JavaScript, lai izveidotu tīmekļa lapas vizuālos elementus, ar kuriem lietotāji mijiedarbojas.\n",
    "\n",
    "\n",
    "### Servera izstrāde - Back End Web Development\n",
    "\n",
    "Servera puses tīmekļa izstrāde ietver servera puses loģikas un datu bāzes mijiedarbības izveidi. Tas ietver lietotāja pieprasījumu apstrādi, datu apstrādi un dinamiskas satura ģenerēšanu. Servera puses izstrādātāji izmanto servera puses programmēšanas valodas, piemēram, Python, PHP, Ruby, Java un citas, lai izveidotu tīmekļa lapas servera puses komponentus.\n",
    "\n",
    "API izstrāde, lietotāja autentifikācijas apstrāde un datu bāzu pārvaldība ir daži no uzdevumiem, par kuriem atbild back end izstrādātāji.\n",
    "\n",
    "### Pilna tīmekļa izstrāde - Full Stack Web Development\n",
    "\n",
    "Pilna tīmekļa izstrāde ietver darbu gan ar front end, gan ar back end tehnoloģijām. Pilna tīmekļa izstrādātāji ir prasmīgi gan front end, gan back end tehnoloģijās un var izveidot pilnīgas tīmekļa lietotnes no sākuma līdz beigām.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask - A Python Web Framework\n",
    "\n",
    "Flask is a lightweight Python web framework that allows you to build web applications quickly and easily. It is designed to be simple and easy to use, making it a great choice for beginners and experienced developers alike.\n",
    "\n",
    "### How Flask Works\n",
    "\n",
    "Flask is a micro web framework that provides the basic tools and libraries needed to build web applications. It is built on top of the WSGI (Web Server Gateway Interface) standard, which allows it to work with a variety of web servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual Environment Setup\n",
    "\n",
    "Before installing Flask, it is HIGHLY recommended to create a virtual environment for your project. This will help you manage dependencies and avoid conflicts with other projects.\n",
    "\n",
    "There are multiple ways to create a virtual environment in Python. One common way is to use the built-in `venv` module. Here's how you can create a virtual environment using `venv`:\n",
    "\n",
    "```bash\n",
    "# Create a new directory for your project\n",
    "mkdir myproject\n",
    "cd myproject\n",
    "python -m venv myvenv\n",
    "```\n",
    "\n",
    "Instead of myenv, you can use any name you like for your virtual environment. To activate the virtual environment, you can use the following command:\n",
    "\n",
    "```bash\n",
    "# On Windows\n",
    "myvenv\\Scripts\\activate\n",
    "\n",
    "# On macOS and Linux\n",
    "source myvenv/bin/activate\n",
    "```\n",
    "\n",
    "### Installing Flask\n",
    "\n",
    "Once you have set up and ACTIVATED your virtual environment, you can install Flask using `pip`:\n",
    "\n",
    "```bash\n",
    "\n",
    "pip install Flask\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple Web Application with Flask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "# let's create a simple Flask app\n",
    "\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__) # this creates a new Flask app object\n",
    "\n",
    "# we will be using app.route() decorator to define the URL that will trigger the function below\n",
    "@app.route('/') # this route means that the function below will be called when the user goes to the root URL of your website\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "# you'd add this line to run the app in script mode\n",
    "# if __name__ == '__main__':\n",
    "#     app.run()\n",
    "\n",
    "app.run() # this is the same as the above line, but it's not recommended to use this in script mode\n",
    "# usually you would not run this from Jupiter notebook, but from a terminal\n",
    "\n",
    "# use Ctrl+C to stop the server on terminal\n",
    "# in Jupyter notebook, you can stop the server by clicking on the stop button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using parameters in routes\n",
    "\n",
    "Next step is to create a simple web application that takes a parameter in the URL and displays it on the page. Here's an example:\n",
    "\n",
    "```python\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/greet/<name>')\n",
    "def greet(name):\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Dec/2024 16:54:53] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 16:54:59] \"GET /greet/Valdis HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 16:55:05] \"GET /greet/LUPython HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 16:55:11] \"GET /greet/LUPython/Latvia HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "# note that the URL is case-sensitive\n",
    "# note the use of <name> in the URL this is a variable part of the URL\n",
    "@app.route('/greet/<name>')\n",
    "def greet(name):\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "# note that only first level of the URL will be caught by this route\n",
    "# so /greet/Janis/Berzins will not work\n",
    "# but /greet/Janis will work\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n",
    "# use Ctrl+C to stop the server on terminal\n",
    "# in Jupyter notebook, you can stop the server by clicking on the stop button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using query parameters\n",
    "\n",
    "Query parameters are another way to pass data to a web application. They are added to the URL after a question mark `?` and are in the form `key=value`. Here's an example:\n",
    "\n",
    "```python\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/sveiks')\n",
    "def greet():\n",
    "    name = request.args.get('name', 'World')\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "```\n",
    "\n",
    "Now you can access the greet route with a query parameter like this: `http://localhost:5000/sveiks?name=Uldis`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Dec/2024 17:07:54] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 17:08:03] \"GET /sveiks?name=Valdis HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "## Using query parameters\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/sveiks')\n",
    "def greet():\n",
    "    name = request.args.get('name', 'Pasaule') #if no name argument is given, we will use 'Pasaule'\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n",
    "# on local server try something like http://127.0.0.1:5000/sveiks?name=Valdis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using templates and static files\n",
    "\n",
    "Generally you do not want to handle HTML in your Python code. Flask allows you to use templates to separate your HTML from your Python code. Flask uses Jinja2 as its template engine.\n",
    "\n",
    "More documentation on Jinja2 can be found [here](https://jinja.palletsprojects.com/en/3.0.x/)\n",
    "\n",
    "Basic idea is you create a folder called `templates` in your project folder and put your HTML files there. Note there is a way to extend templates and use variables in them.\n",
    "\n",
    "Also you might have some static files like CSS, JavaScript, images etc. You can put them in a folder called `static` in your project folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Dec/2024 18:24:57] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:24:57] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:24:57] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:00] \"GET /static/mystyle.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:12] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:12] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:12] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:13] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:13] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:13] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:15] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:15] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:15] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:17] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:17] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:17] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:18] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:18] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:18] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:21] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:21] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:21] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /static/myscript.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    items = [\"Saraksti\", \"Vārdnīcas\", \"Citi objekti\"] # these could be from a database or other data source\n",
    "    # also these objects could be from url query parameters\n",
    "    return render_template('index.html', year=2024, items=items)\n",
    "\n",
    "@app.route('/about')\n",
    "def about():\n",
    "    return render_template('about.html', year=2024)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # app.run(debug=True) # debug mode will reload the server on code changes and provide more verbose output\n",
    "    app.run() # debug mode will reload the server on code changes and provide more verbose output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask project structure\n",
    "\n",
    "```\n",
    "project_root/\n",
    "│\n",
    "├── app.py                # Your main Flask application file\n",
    "├── static/               # Folder for static files (CSS, JS, images, etc.)\n",
    "│   └── styles.css        # Your CSS file\n",
    "|   └── script.js         # Your JavaScript file\n",
    "├── templates/            # Folder for HTML templates\n",
    "│   └── base.html         # Your base template\n",
    "│   └── index.html        # Other templates\n",
    "└── requirements.txt      # (Optional) Python dependencies file\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask Learning References \n",
    "\n",
    "*What else you need to learn about Flask:*\n",
    "\n",
    "- Handling forms with Flask\n",
    "- Using databases with Flask\n",
    "- User authentication with Flask\n",
    "- Deploying Flask applications - AWS, PythonAnywhere, DigitalOcean, local server etc.\n",
    "\n",
    "For further exploration of Flask, you can refer to the following resources:\n",
    "- Official Flask Documentation: https://flask.palletsprojects.com/en/2.0.x/\n",
    "- Miguel Grinberg's Flask Mega-Tutorial: https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world\n",
    "- Corey Schafer's Flask Tutorial Series: https://www.youtube.com/playlist?list=PL-osiE80TeTs4UjLw5MM6OjgkjFeUxCYH\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping - what is it?\n",
    "\n",
    "Web scraping is the process of extracting data from websites. It involves sending HTTP requests to a website, parsing the HTML content, and extracting the data you need. Web scraping is commonly used for data mining, price monitoring, and content aggregation.\n",
    "\n",
    "Web scraping can be done manually by simply saving contents of web site you are visiting, but it is often more efficient to use a web scraping library like Scrapy to automate the process.\n",
    "\n",
    "### Rules of Web Scraping\n",
    "\n",
    "* Play nice! Don't overload a website with requests, as this can cause server issues and get you banned.\n",
    "\n",
    "* Before scraping a website, make sure to check the website's terms of service and robots.txt file to ensure that you are not violating any rules or policies.\n",
    "\n",
    "* Also if it is possible to obtain the data you need through an API, it is recommended to use the API instead of scraping the website.\n",
    "\n",
    "* Even better if data can be obtained from a public dataset, it is recommended to use the dataset instead of scraping the website.\n",
    "\n",
    "* Note the difference between scraping something and using the data. Scraping is just the process of extracting data from a website, while using the data is a separate issue. For example there is a difference between doing a research on some website and using the same data to setup a competing service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy - A Python Web Scraping Library\n",
    "\n",
    "Scrapy is a powerful web scraping library for Python that makes it easy to extract data from websites. It provides a high-level API for crawling websites and extracting data, making it a great choice for web scraping projects.\n",
    "\n",
    "### How Scrapy Works\n",
    "\n",
    "Scrapy works by sending HTTP requests to a website, parsing the HTML content, and extracting the data you need. It provides a set of tools and libraries for building web scrapers, including a built-in web crawler, a powerful selector system, and support for handling cookies and sessions.\n",
    "\n",
    "### Installing Scrapy\n",
    "\n",
    "As usual it is best to create and activate a virtual environment before installing Scrapy. \n",
    "\n",
    "You can install Scrapy using `pip`:\n",
    "\n",
    "```bash\n",
    "pip install Scrapy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple web scraping example with Scrapy\n",
    "\n",
    "Let's say we want to scrape the cities and their populations from wikipedia page: https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\n",
    "(Note that Wikipedia offers an API for accessing its data, so scraping is not necessary in this case. This is just an example.)\n",
    "\n",
    "Here is a simple example of how to scrape data from a website using Scrapy:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy version is 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# first let's import scrapy and check its version\n",
    "try:\n",
    "    import scrapy\n",
    "    print(f\"Scrapy version is {scrapy.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Scrapy is not installed\")\n",
    "    print(\"You can install Scrapy with pip install scrapy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic scraping example with Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 19:38:03 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): en.wikipedia.org:443\n",
      "2024-12-09 19:38:03 [urllib3.connectionpool] DEBUG: https://en.wikipedia.org:443 \"GET /wiki/List_of_cities_in_Latvia HTTP/11\" 200 38490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will scrape https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\n",
      "Web page title is List of cities and towns in Latvia - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "# now let's scrape the following web page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_cities_in_Latvia'\n",
    "print(f\"We will scrape {url}\")\n",
    "# Note: usually there is no need to scrape Wikipedia as they have APIs and data dumps available\n",
    "# but for learning purposes we will scrape this page\n",
    "# we are interested in table with cities and their population\n",
    "\n",
    "# we've already imported scrapy so we can start using it\n",
    "# let's start with basic example where we simply fetch the page and extract the title\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "try:\n",
    "    import requests  # we also use requests library to fetch the page\n",
    "except ImportError:\n",
    "    print(\"Requests is not installed\")\n",
    "    print(\"You can install Requests with pip install requests\")\n",
    "\n",
    "# NOTE: Scrapy has its own request object as well, but it is more complex\n",
    "\n",
    "response = requests.get(url)\n",
    "# let's create a scrapy response object\n",
    "scrapy_response = HtmlResponse(url, body=response.text, encoding='utf-8')\n",
    "\n",
    "# let's extract the title using css selector\n",
    "title = scrapy_response.css('title::text').get()\n",
    "print(\"Web page title is\", title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CSS selectors to extract data\n",
    "\n",
    "Scrapy provides a powerful selector system that allows you to extract data from HTML using CSS selectors. You can use CSS selectors to target specific elements on a page and extract the data you need.\n",
    "\n",
    " **CSS Selectors** CSS Selectors allow for simpler and often more intuitive queries using CSS rules. Scrapy's `response.css()` method provides an interface for selecting elements based on CSS selectors.\n",
    " \n",
    "#### Common CSS Examples: \n",
    "| CSS Selector | Description | \n",
    "| --- | --- | \n",
    "| tag | Selects all <tag> elements. | \n",
    "| .class | Selects all elements with the class class. | \n",
    "| #id | Selects the element with the ID id. | \n",
    "| tag.class | Selects <tag> elements with the class class. | \n",
    "| tag[attr=\"value\"] | Selects <tag> elements with an attribute attr=\"value\". | \n",
    "| tag > child | Selects direct children of <tag>. | \n",
    "| tag child | Selects all descendants of <tag>. | \n",
    "| tag:first-child | Selects the first child <tag> of its parent. | \n",
    "| tag:nth-child(n) | Selects the nth <tag> child. | \n",
    "\n",
    "#### Scrapy CSS Methods: \n",
    " \n",
    "- `response.css('<CSS selector>')`: Extracts elements matching the CSS selector.\n",
    " \n",
    "- `.get()`: Returns the first matching value.\n",
    " \n",
    "- `.getall()`: Returns all matching values as a list.\n",
    " \n",
    "- `.re('<regex>')`: Extracts values matching a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 tables on the page\n",
      "There are 2 wikitable sortable tables on the page\n",
      "Table(s) have rows 83\n"
     ]
    }
   ],
   "source": [
    "# now let's use Selector to get the table with cities and their population\n",
    "# we will use CSS selectors\n",
    "# we will use the table with class wikitable sortable\n",
    "\n",
    "# first let's see about getting all tables\n",
    "tables = scrapy_response.css('table')\n",
    "# how many tables are there?\n",
    "print(\"There are\", len(tables), \"tables on the page\")\n",
    "\n",
    "# by using inspect element in browser we can see that the table we want is the first table with class wikitable sortable\n",
    "table = scrapy_response.css('table.wikitable.sortable') \n",
    "# how many tables are there?\n",
    "print(\"There are\", len(table), \"wikitable sortable tables on the page\")\n",
    "# in our case this is fine as we want elements from both tables\n",
    "# if we only needed the first table we could use table = scrapy_response.css('table.wikitable.sortable')[0] as the first table is at index 0\n",
    "\n",
    "# how many tr elements are in the tables\n",
    "rows = table.css('tr')\n",
    "print(\"Table(s) have rows\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest five cities [{'city': ['Rīga', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '658,640\\n'}, {'city': ['Daugavpils', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '93,312\\n'}, {'city': ['Liepāja', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '76,731\\n'}, {'city': ['Jelgava', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '59,511\\n'}, {'city': ['Jūrmala', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '50,840\\n'}]\n",
      "Smallest five cities [{'city': ['Varakļāni', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '2,106\\n'}, {'city': ['Viesīte', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '1,902\\n'}, {'city': ['Viļaka', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '1,525\\n'}, {'city': ['Viļāni', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '3,468\\n'}, {'city': ['Zilupe', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '1,746\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# let's save data in a list of dictionaries for easier processing\n",
    "cities = []\n",
    "# let's iterate over rows and extract the data\n",
    "for row in rows:\n",
    "    # city = row.css('td:nth-child(1)::text').get()\n",
    "    # we want all text from first child td element that is contained in its children\n",
    "    city = row.css('td:nth-child(1) *::text').getall() # note the * after td:nth-child(1) we get all text from all children\n",
    "    population = row.css('td:nth-child(2)::text').get() # here we get only the text from the td element\n",
    "    if city and population: # we only want rows with city and population\n",
    "        cities.append({'city': city, 'population': population})\n",
    "# print first 5 cities\n",
    "print(\"Biggest five cities\", cities[:5])\n",
    "# last 5 cities\n",
    "print(\"Smallest five cities\", cities[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest five cities [{'city': 'Rīga', 'population': '658,640\\n'}, {'city': 'Daugavpils', 'population': '93,312\\n'}, {'city': 'Liepāja', 'population': '76,731\\n'}, {'city': 'Jelgava', 'population': '59,511\\n'}, {'city': 'Jūrmala', 'population': '50,840\\n'}]\n",
      "Smallest five cities [{'city': 'Varakļāni', 'population': '2,106\\n'}, {'city': 'Viesīte', 'population': '1,902\\n'}, {'city': 'Viļaka', 'population': '1,525\\n'}, {'city': 'Viļāni', 'population': '3,468\\n'}, {'city': 'Zilupe', 'population': '1,746\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# we can see that we only care about the first entry for city name so let's clean up the list of dictionaries\n",
    "# we will keep only the first entry for city name\n",
    "cities = [{'city': d['city'][0], 'population': d['population']} for d in cities] # list comprehension\n",
    "# print first 5 cities\n",
    "print(\"Biggest five cities\", cities[:5])\n",
    "# last 5 cities\n",
    "print(\"Smallest five cities\", cities[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we could save the results to a file or database\n",
    "# alternatively we could load the data into a pandas DataFrame for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using XPath selectors to extract data\n",
    "\n",
    "XPath is more powerful than CSS selectors, but it is also more complex. \n",
    "\n",
    "**XPath Selectors** XPath (XML Path Language) allows you to navigate and query nodes in an HTML or XML document. Scrapy's `response.xpath()` method provides an interface for selecting elements based on XPath expressions.\n",
    "\n",
    "#### Common XPath Examples: \n",
    "| XPath Expression | Description | \n",
    "| --- | --- | \n",
    "| //tag | Selects all <tag> elements anywhere in the document. | \n",
    "| ./tag | Selects all <tag> elements directly under the current node. | \n",
    "| //tag[@attr=\"value\"] | Selects <tag> elements with an attribute attr=\"value\". | \n",
    "| //tag/text() | Selects the text content of <tag> elements. | \n",
    "| //tag[contains(@attr, \"val\")] | Selects <tag> elements where attr contains \"val\". | \n",
    "| //tag[1] | Selects the first <tag> element in the context. | \n",
    "| //tag[last()] | Selects the last <tag> element in the context. | \n",
    "| //tag[position() < 3] | Selects the first two <tag> elements. | \n",
    "\n",
    "#### Scrapy XPath Methods: \n",
    " \n",
    "- `response.xpath('<XPath>')`: Extracts elements matching the XPath.\n",
    " \n",
    "- `.get()`: Returns the first matching value.\n",
    " \n",
    "- `.getall()`: Returns all matching values as a list.\n",
    " \n",
    "- `.extract()`: Deprecated alias for `.getall()`.\n",
    " \n",
    "- `.re('<regex>')`: Extracts values matching a regular expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 wikitable sortable tables on the page\n"
     ]
    }
   ],
   "source": [
    "# now let's see how we could have used XPath selectors to get the same data\n",
    "\n",
    "# let's extract the table with XPath\n",
    "table = scrapy_response.xpath('//table[contains(@class, \"wikitable\") and contains(@class, \"sortable\")]')\n",
    "# how many tables are there?\n",
    "print(\"There are\", len(table), \"wikitable sortable tables on the page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table(s) have rows 83\n"
     ]
    }
   ],
   "source": [
    "# now let's extract the rows with XPath\n",
    "rows = table.xpath('.//tr')\n",
    "print(\"Table(s) have rows\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try something slightly fancier we will want to extract text from first anchor child in first td element - city name\n",
    "# example\n",
    "# <tr>\n",
    "# <td><a href=\"/wiki/R%C4%ABga\" class=\"mw-redirect\" title=\"Rīga\">Rīga</a>&nbsp;<small><span class=\"noprint\"><span class=\"ext-phonos\"><span data-nosnippet=\"\" id=\"ooui-php-1\" class=\"ext-phonos-PhonosButton noexcerpt oo-ui-widget oo-ui-widget-enabled oo-ui-buttonElement oo-ui-buttonElement-frameless oo-ui-iconElement oo-ui-labelElement oo-ui-buttonWidget\" data-ooui=\"{&quot;_&quot;:&quot;mw.Phonos.PhonosButton&quot;,&quot;href&quot;:&quot;\\/\\/upload.wikimedia.org\\/wikipedia\\/commons\\/transcoded\\/f\\/f5\\/Lv-R%C4%ABga.ogg\\/Lv-R%C4%ABga.ogg.mp3&quot;,&quot;rel&quot;:[&quot;nofollow&quot;],&quot;framed&quot;:false,&quot;icon&quot;:&quot;volumeUp&quot;,&quot;label&quot;:{&quot;html&quot;:&quot;pronunciation&quot;},&quot;data&quot;:{&quot;ipa&quot;:&quot;&quot;,&quot;text&quot;:&quot;&quot;,&quot;lang&quot;:&quot;en&quot;,&quot;wikibase&quot;:&quot;&quot;,&quot;file&quot;:&quot;Lv-R\\u012bga.ogg&quot;},&quot;classes&quot;:[&quot;ext-phonos-PhonosButton&quot;,&quot;noexcerpt&quot;]}\"><a role=\"button\" tabindex=\"0\" href=\"//upload.wikimedia.org/wikipedia/commons/transcoded/f/f5/Lv-R%C4%ABga.ogg/Lv-R%C4%ABga.ogg.mp3\" rel=\"nofollow\" aria-label=\"Play audio\" title=\"Play audio\" class=\"oo-ui-buttonElement-button\"><span class=\"oo-ui-iconElement-icon oo-ui-icon-volumeUp\"></span><span class=\"oo-ui-labelElement-label\">pronunciation</span><span class=\"oo-ui-indicatorElement-indicator oo-ui-indicatorElement-noIndicator\"></span></a></span><sup class=\"ext-phonos-attribution noexcerpt navigation-not-searchable\"><a href=\"/wiki/File:Lv-R%C4%ABga.ogg\" title=\"File:Lv-Rīga.ogg\">ⓘ</a></sup></span></span></small>&nbsp;<figure class=\"mw-halign-right\" typeof=\"mw:File\"><a href=\"/wiki/File:Greater_Coat_of_Arms_of_Riga_-_for_display.svg\" class=\"mw-file-description\"><img src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Greater_Coat_of_Arms_of_Riga_-_for_display.svg/55px-Greater_Coat_of_Arms_of_Riga_-_for_display.svg.png\" decoding=\"async\" width=\"55\" height=\"33\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Greater_Coat_of_Arms_of_Riga_-_for_display.svg/83px-Greater_Coat_of_Arms_of_Riga_-_for_display.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/99/Greater_Coat_of_Arms_of_Riga_-_for_display.svg/110px-Greater_Coat_of_Arms_of_Riga_-_for_display.svg.png 2x\" data-file-width=\"510\" data-file-height=\"303\"></a><figcaption></figcaption></figure>\n",
    "# </td>\n",
    "# <td>658,640\n",
    "# </td>\n",
    "# <td>632,614\n",
    "# </td>\n",
    "# <td>605,273\n",
    "# </td></tr>\n",
    "\n",
    "# here we can see that city name Rīga is in the first td element inside its first anchor child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest five cities [{'city': 'Rīga', 'population': '658,640\\n'}, {'city': 'Daugavpils', 'population': '93,312\\n'}, {'city': 'Liepāja', 'population': '76,731\\n'}, {'city': 'Jelgava', 'population': '59,511\\n'}, {'city': 'Jūrmala', 'population': '50,840\\n'}]\n",
      "Smallest five cities [{'city': 'Varakļāni', 'population': '2,106\\n'}, {'city': 'Viesīte', 'population': '1,902\\n'}, {'city': 'Viļaka', 'population': '1,525\\n'}, {'city': 'Viļāni', 'population': '3,468\\n'}, {'city': 'Zilupe', 'population': '1,746\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# now let's save data in a list of dictionaries for easier processing\n",
    "also_cities = []\n",
    "# let's iterate over rows and extract the data\n",
    "for row in rows:\n",
    "    # city = row.xpath('.//td[1]//text()').getall() # we want all text from first child td element that is contained in its children\n",
    "    # we want text from first anchor element in td[1]\n",
    "    city = row.xpath('.//td[1]//a[1]//text()').get() # note the //a[1] we get text from first anchor child of the first td element\n",
    "    population = row.xpath('.//td[2]//text()').get() # here we get only the text from the td element\n",
    "    if city and population: # we only want rows with city and population\n",
    "        also_cities.append({'city': city, 'population': population})\n",
    "# print first 5 cities\n",
    "print(\"Biggest five cities\", also_cities[:5])\n",
    "# last 5 cities\n",
    "print(\"Smallest five cities\", also_cities[-5:])\n",
    "\n",
    "# using XPath we did not have to do any Python list comprehension to clean up the data after extraction\n",
    "# now we could still clean up newlines and convert population to integer but that is easy and not part of scraping itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS vs XPath key differences in Scrapy\n",
    "\n",
    "| Feature | XPath | CSS | \n",
    "| --- | --- | --- | \n",
    "| Syntax Complexity | More expressive and versatile | Simpler, easier to write | \n",
    "| Attribute Matching | @attr | [attr=\"value\"] | \n",
    "| Text Content | .//text() | ::text | \n",
    "| Positional Matching | [position()=n] | :nth-child(n) | \n",
    "| Advanced Navigation | Supports parent/sibling navigation | Limited to child navigation | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping multiple pages with Scrapy\n",
    "\n",
    "For scraping multiple pages, you can use Scrapy's built-in web crawler to follow links and scrape data from multiple pages. You can define rules to follow links and extract data from each page.\n",
    "\n",
    "For now we will use preset list of URLs to scrape, but in real life you would probably want to scrape links from the page itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will scrape the following pages:\n",
      "https://en.wikipedia.org/wiki/List_of_cities_in_Estonia\n",
      "https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\n",
      "https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania\n"
     ]
    }
   ],
   "source": [
    "start_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Estonia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania\",\n",
    "]\n",
    "print(\"We will scrape the following pages:\", *start_urls, sep=\"\\n\") # we using * to unpack the list into separate arguments for neat printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:14:36 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: scrapybot)\n",
      "2024-12-09 20:14:36 [scrapy.utils.log] INFO: Versions: lxml 5.2.2.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.11.0, Python 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)], pyOpenSSL 24.3.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Windows-11-10.0.22631-SP0\n",
      "2024-12-09 20:14:36 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-12-09 20:14:36 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-12-09 20:14:36 [scrapy.extensions.telnet] INFO: Telnet Password: 230407da82ccaf53\n",
      "2024-12-09 20:14:36 [py.warnings] WARNING: c:\\pyenvs\\venv312\\Lib\\site-packages\\scrapy\\extensions\\feedexport.py:432: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n",
      "2024-12-09 20:14:36 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-12-09 20:14:36 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-12-09 20:14:37 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-12-09 20:14:37 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-12-09 20:14:37 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-12-09 20:14:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-12-09 20:14:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_cities_in_Estonia> (referer: None)\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania> (referer: None)\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_cities_in_Latvia> (referer: None)\n",
      "2024-12-09 20:14:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/List_of_cities_in_Estonia>\n",
      "{'title': 'List of cities and towns in Estonia - Wikipedia'}\n",
      "2024-12-09 20:14:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania>\n",
      "{'title': 'List of cities in Lithuania - Wikipedia'}\n",
      "2024-12-09 20:14:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/List_of_cities_in_Latvia>\n",
      "{'title': 'List of cities and towns in Latvia - Wikipedia'}\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-12-09 20:14:37 [scrapy.extensions.feedexport] INFO: Stored json feed (3 items) in: cities_titles.json\n",
      "2024-12-09 20:14:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 760,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 119110,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 3,\n",
      " 'elapsed_time_seconds': 0.306788,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 12, 9, 18, 14, 37, 551379, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 819985,\n",
      " 'httpcompression/response_count': 3,\n",
      " 'item_scraped_count': 3,\n",
      " 'items_per_minute': None,\n",
      " 'log_count/DEBUG': 7,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 3,\n",
      " 'responses_per_minute': None,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2024, 12, 9, 18, 14, 37, 244591, tzinfo=datetime.timezone.utc)}\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# we have the start_urls now we can create a Scrapy spider to scrape these pages\n",
    "# in this case we will use a simple spider that will only extract the title of the page\n",
    "\n",
    "# let's do a simple spider that will extract the title of the page\n",
    "\n",
    "# we want to save these titles in a list of dictionaries as JSON\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class SimpleSpider(scrapy.Spider):\n",
    "    name = 'simple_spider'\n",
    "    start_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Estonia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania\",\n",
    "] # we could have passed this as a parameter to the spider\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').get()\n",
    "        # ADD your own code here to extract more data from each page\n",
    "        # you can use either css or xpath selectors\n",
    "        yield {'title': title}\n",
    "\n",
    "# let's run the spider\n",
    "# we also want to save the results to a file\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    'FEED_URI': 'cities_titles.json',\n",
    "    'FEED_FORMAT': 'json'\n",
    "})\n",
    "\n",
    "process.crawl(SimpleSpider)\n",
    "process.start()\n",
    "# NOTE this process is really meant to be run from a script or terminal\n",
    "# if you run it in a Jupyter notebook you might have to restart the kernel to run it again\n",
    "# otherwise you will get errors about already running Twisted reactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi page scraper overview\n",
    "\n",
    " \n",
    "1. **Create the Spider** : \n",
    "  - Include all URLs in `start_urls` or load them dynamically using `start_requests()`.\n",
    " \n",
    "2. **Parse Each Page** : \n",
    "  - Use the same `parse()` method for extracting data since the structure of the pages is similar.\n",
    " \n",
    "3. **Identify the Context** :\n",
    "  - Use information from the URL or page content to tag the data (e.g., country name).\n",
    " \n",
    "4. **Run the Spider** :\n",
    "  - Execute the spider and save the output to a file.\n",
    " \n",
    "5. **Post-Processing** :\n",
    "  - Optionally, combine or clean the scraped data further (e.g., merge JSON files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy Learning References\n",
    "\n",
    "For further exploration of Scrapy, you can refer to the following resources:\n",
    "\n",
    "- Official Scrapy Documentation: https://docs.scrapy.org/en/latest/\n",
    "- Scrapy Tutorial: https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    "- YouTube Tutorial: https://www.youtube.com/watch?v=ve_0h4Y8nuI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "### Flask Practice\n",
    "\n",
    "1. Create a simple web application using Flask that displays a list of items. The list should be stored in a Python list and displayed on the web page.\n",
    "2. Add a form to the web application that allows users to add new items to the list.\n",
    "3. Add a delete button next to each item in the list that allows users to delete items from the list.\n",
    "\n",
    "### Scrapy Practice\n",
    "\n",
    "1. Create a Scrapy spider that scrapes data from a website of your choice. The spider should extract at least two fields from the website and save the data to a JSON or CSV file.\n",
    "2. Modify the spider to save the data to a database instead of a JSON or CSV file.\n",
    "3. Add error handling to the spider to handle cases where the website is down or the data is missing.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, we covered the basics of server-side web development using Flask and web scraping using Scrapy. We learned how to create a simple web application with Flask and how to scrape data from a website using Scrapy. We also discussed the rules of web scraping and best practices for working with web scraping libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
