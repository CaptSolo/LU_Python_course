{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LU Logo](https://www.lu.lv/fileadmin/user_upload/LU.LV/www.lu.lv/Logo/Logo_jaunie/LU_logo_LV_horiz.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tīmekļa lapu izstrāde ar Flask un datu iegūšana(rasmošana) ar Scrapy\n",
    "\n",
    "## Nodarbības saturs\n",
    "\n",
    "Mēs apskatīsim šādas tēmas šajā nodarbībā:\n",
    "\n",
    "* Flask - Python servera puses tīmekļa izstrādes rīks jeb sastatnes\n",
    "* Scrapy - Python bibliotēka datu iegūšanai no tīmekļa lapām, jeb rasmošanai\n",
    "\n",
    "## Nodarbības mērķi\n",
    "\n",
    "Nodarbības beigās jūs būsiet spējīgi:\n",
    "\n",
    "* Saprast servera puses tīmekļa izstrādes pamatus\n",
    "* Izveidot vienkāršu tīmekļa lietotni, izmantojot Flask\n",
    "* Saprast tīmekļa lapu rasmošanas pamatus\n",
    "* Iegūt datus no tīmekļa lapas, izmantojot Scrapy\n",
    "\n",
    "## Nepieciešamās priekšzināšanas\n",
    "\n",
    "Pirms sākat šo nodarbību, jums vajadzētu:\n",
    "\n",
    "* Saprast Python programmēšanas pamatus - mainīgie, datu tipi, kontroles struktūras, funkcijas, OOP un failu ievade/izvade\n",
    "* Zināt, kā instalēt Python pakotnes, izmantojot `pip`\n",
    "* Zināt, kā izveidot virtuālo vidi, izmantojot `venv`\n",
    "* Saprast HTML pamatus - skatiet MDN Web Docs [HTML ievads](https://developer.mozilla.org/en-US/docs/Learn/HTML/Introduction_to_HTML)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tīmekļa lapu izstrādes pamati\n",
    "\n",
    "Tīmekļa lapu izstrāde ir plašs jēdziens, kas ietver daudzas dažādas tehnoloģijas un prasmes.\n",
    "\n",
    "Viens dalījums būtu pēc tā, vai jūs strādājat ar lietotāja saskarni (front end) vai ar servera pusi (back end).\n",
    "\n",
    "\n",
    "### Lietotājā saskarne - Front End Web Development\n",
    "\n",
    "Lietotāja saskarnes izstrāde ietver tīmekļa lapas lietotāja saskarnes un lietotāja pieredzes(UX) izveidi. Tas ietver tīmekļa lapas izkārtojuma, krāsu, fontu un interaktīvo elementu projektēšanu. Front end izstrādātāji izmanto HTML, CSS un JavaScript, lai izveidotu tīmekļa lapas vizuālos elementus, ar kuriem lietotāji mijiedarbojas.\n",
    "\n",
    "\n",
    "### Servera izstrāde - Back End Web Development\n",
    "\n",
    "Servera puses tīmekļa izstrāde ietver servera puses loģikas un datu bāzes mijiedarbības izveidi. Tas ietver lietotāja pieprasījumu apstrādi, datu apstrādi un dinamiskas satura ģenerēšanu. Servera puses izstrādātāji izmanto servera puses programmēšanas valodas, piemēram, Python, PHP, Ruby, Java un citas, lai izveidotu tīmekļa lapas servera puses komponentus.\n",
    "\n",
    "API izstrāde, lietotāja autentifikācijas apstrāde un datu bāzu pārvaldība ir daži no uzdevumiem, par kuriem atbild back end izstrādātāji.\n",
    "\n",
    "### Pilna tīmekļa izstrāde - Full Stack Web Development\n",
    "\n",
    "Pilna tīmekļa izstrāde ietver darbu gan ar front end, gan ar back end tehnoloģijām. Pilna tīmekļa izstrādātāji ir prasmīgi gan front end, gan back end tehnoloģijās un var izveidot pilnīgas tīmekļa lietotnes no sākuma līdz beigām.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask - Python tīmekļa izstrādes rīks\n",
    "\n",
    "Flask ir vienkāršs un viegli lietojams Python tīmekļa izstrādes rīks. Tas ir paredzēts, lai palīdzētu jums izveidot tīmekļa lietotnes ātri un vienkārši.\n",
    "To izmanto gan pieredzējuši izstrādātāji, gan iesācēji, jo tas ir viegli saprotams un lietojams.\n",
    "\n",
    "### Kā darbojas Flask\n",
    "\n",
    "\n",
    "Flask darbojas, izmantojot WSGI (Web Server Gateway Interface) standartu, kas ļauj to darboties ar dažādiem tīmekļa serveriem. Tas nozīmē, ka jūs varat izmantot Flask ar dažādiem tīmekļa serveriem, piemēram, Apache, Nginx vai citiem.\n",
    "\n",
    "Flask izmanto dekoratorus, lai definētu maršrutus un funkcijas, kas tiek izpildītas, kad tiek saņemts pieprasījums uz konkrēto maršrutu. Tas ļauj jums viegli definēt, kāda darbība jāveic, kad tiek saņemts pieprasījums uz konkrēto URL. Tos apskatīsim vēlāk šajā nodarbībā."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtuālās vides iestatīšana\n",
    "\n",
    "Pirms instalējat Flask, IZSTRĀDĀTĀJIEM ĻOTI IESAKĀMS izveidot virtuālo vidi savam projektam. Tas palīdzēs jums pārvaldīt atkarības un izvairīties no konfliktiem ar citiem projektiem.\n",
    "\n",
    "Pastāv vairāki veidi, kā izveidot virtuālo vidi Python. Viena no visbiežāk izmantotajām metodēm ir izmantot iebūvēto `venv` moduli. Šādi varat izveidot virtuālo vidi, izmantojot `venv`:\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create a new directory for your project\n",
    "mkdir myproject\n",
    "cd myproject\n",
    "python -m venv myvenv\n",
    "```\n",
    "\n",
    "Tā vietā, lai izmantotu myenv, varat izmantot jebkuru vēlamo nosaukumu savai virtuālajai videi. \n",
    "\n",
    "#### Virtuālās vides aktivizēšana\n",
    "\n",
    "Lai aktivizētu virtuālo vidi, varat izmantot šādu komandu:\n",
    "\n",
    "```bash\n",
    "# On Windows\n",
    "myvenv\\Scripts\\activate\n",
    "\n",
    "# On macOS and Linux\n",
    "source myvenv/bin/activate\n",
    "```\n",
    "\n",
    "### Flask uzstādīšana\n",
    "\n",
    "Kad esat izveidojis un AKTIVIZĒJIS savu virtuālo vidi, varat instalēt Flask, izmantojot `pip`:\n",
    "\n",
    "```bash\n",
    "\n",
    "pip install Flask\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veidojam vienkāršu tīmekļa lietotni ar Flask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello Pasaule piemērs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "# izveidosim vienkāršu Flask aplikāciju\n",
    "\n",
    "from flask import Flask # import the Flask class from the flask module\n",
    "\n",
    "app = Flask(__name__) # this creates a new Flask app object\n",
    "\n",
    "# we will be using app.route() decorator to define the URL that will trigger the function below\n",
    "@app.route('/') # this route means that the function below will be called when the user goes to the root URL of your website\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "# you'd add this line to run the app in script mode\n",
    "# if __name__ == '__main__':\n",
    "#     app.run()\n",
    "\n",
    "app.run() # this is the same as the above line, but it's not recommended to use this in script mode\n",
    "# usually you would not run this from Jupiter notebook, but from a terminal\n",
    "\n",
    "# use Ctrl+C to stop the server on terminal\n",
    "# in Jupyter notebook, you can stop the server by clicking on the stop button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametru izmantošana maršrutos\n",
    "\n",
    "Nākošais solis ir izveidot vienkāršu tīmekļa lietotni, kas ņem parametru URL un parāda to lapā. Šeit ir piemērs:\n",
    "\n",
    "\n",
    "```python\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/greet/<name>')\n",
    "def greet(name):\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Dec/2024 16:54:53] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 16:54:59] \"GET /greet/Valdis HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 16:55:05] \"GET /greet/LUPython HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 16:55:11] \"GET /greet/LUPython/Latvia HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "# note that the URL is case-sensitive\n",
    "# note the use of <name> in the URL this is a variable part of the URL\n",
    "@app.route('/greet/<name>')\n",
    "def greet(name):\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "# note that only first level of the URL will be caught by this route\n",
    "# so /greet/Janis/Berzins will not work - you would need a separate route for that\n",
    "# but /greet/Janis will work\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n",
    "# use Ctrl+C to stop the server on terminal\n",
    "# in Jupyter notebook, you can stop the server by clicking on the stop button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pieprasījumu parametru izmantošana\n",
    "\n",
    "Pieprasījuma parametri ir vēl viens veids, kā padot datus tīmekļa lietotnei. Tos pievieno URL pēc jautājuma zīmes `?` un ir formā `key=value`. Šeit ir piemērs:\n",
    "\n",
    "```python\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/sveiks')\n",
    "def greet():\n",
    "    name = request.args.get('name', 'World')\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "```\n",
    "\n",
    "Tagad varat piekļūt sveiks maršrutam ar pieprasījuma parametru šādi: `http://localhost:5000/sveiks?name=Uldis`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Dec/2024 17:07:54] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 17:08:03] \"GET /sveiks?name=Valdis HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "## Using query parameters\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/sveiks')\n",
    "def greet():\n",
    "    name = request.args.get('name', 'Pasaule') #if no name argument is given, we will use 'Pasaule'\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n",
    "# on local server try something like http://127.0.0.1:5000/sveiks?name=Valdis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Šablonu un statisko failu izmantošana\n",
    "\n",
    "Parasti mēs nevēlamies apstrādāt HTML savā Python kodā. Flask ļauj izmantot šablonus, lai atdalītu HTML no Python koda. Flask izmanto Jinja2 kā savu šablonu dzinēju.\n",
    "\n",
    "Pilna dokumentācija par Jinja2 atrodama [šeit](https://jinja.palletsprojects.com/en/3.0.x/)\n",
    "\n",
    "Pamatideja ir tāda, ka izveidojat mapi `templates` savam projektam un ievietojat HTML failus tur. Ievērosim ka šabloni ļaus mums ievietot mainīgos un izteiksmes, kas tiks aizstātas ar reāliem datiem, kad šablons tiks renderēts.\n",
    "\n",
    "Papildus dinamiskam saturam, jums var būt nepieciešami arī statiskie faili, piemēram, CSS, JavaScript, attēli utt. Lai tos iekļautu savā projektā, izveidojiet mapi `static` un ievietojiet failus tur.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Dec/2024 18:24:57] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:24:57] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:24:57] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:00] \"GET /static/mystyle.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:12] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:12] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:12] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:13] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:13] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:13] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:15] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:15] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:15] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:17] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:17] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:17] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:18] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:18] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:18] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:21] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:21] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:21] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /static/myscript.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# example of using templates and static files\n",
    "from flask import Flask, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    items = [\"Saraksti\", \"Vārdnīcas\", \"Citi objekti\"] # these could be from a database or other data source\n",
    "    # also these objects could be from url query parameters\n",
    "    return render_template('index.html', year=2024, items=items) # thus template will receive year and items\n",
    "# see index.html templates folder on how it is handled on the template side\n",
    "# you might also check out base.html to see how templates can be extended\n",
    "\n",
    "@app.route('/about')\n",
    "def about():\n",
    "    return render_template('about.html', year=2024)\n",
    "# about.html is even simpler template than index.html\n",
    "# it also extends base.html in templates folder\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # app.run(debug=True) # debug mode will reload the server on code changes and provide more verbose output\n",
    "    app.run() # debug mode will reload the server on code changes and provide more verbose output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask projektu struktūra\n",
    "\n",
    "Lai jūsu Flask projekts būtu labi organizēts, jūs varat izmantot šādu struktūru:\n",
    "\n",
    "```\n",
    "project_root/\n",
    "│\n",
    "├── app.py                # Jūsu galvenais Flask fails - iespējams, ka būs arī citi .py faili\n",
    "├── static/               # Mape statiskiem failiem( CSS, JavaScript, attēli, utt.)\n",
    "│   └── styles.css        # Jūsu CSS fails - var protams būt vairāki\n",
    "|   └── script.js         # Jūsu JavaScript fails - var protams būt vairāki\n",
    "├── templates/            # Mape šabloniem\n",
    "│   └── base.html         # Bāzes šablons - var protams būt vairāki\n",
    "│   └── index.html        # Citi šabloni - var protams būt vairāki\n",
    "└── requirements.txt      # (Ieteicams bet ne obligāts) saraksts ar visām nepieciešamajām bibliotēkām\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mācību materiāli un resursi\tFlask apguvei\n",
    "\n",
    "*Lai būtu pilnvērtīgs Flask izstrādātājs, jums būs nepieciešams iemācīties vairāk par šādām tēmām:*\t\n",
    "\n",
    "- Formu apstrāde ar Flask\n",
    "- Datu bāzu izmantošana ar Flask, SQLAlchemy vai citiem ORM - Object-Relational Mapping rīkiem\n",
    "- Lietotāja autentifikācija ar Flask\n",
    "- Sesiju pārvaldība ar Flask\n",
    "- Flask lietotņu izvietošana - AWS, PythonAnywhere, DigitalOcean, lokālais serveris utt.\n",
    "\n",
    "Lai turpinātu apgūt Flask, jūs varat izmantot šādus resursus:\n",
    "- Oficiālā Flask dokumentācija: https://flask.palletsprojects.com/en/2.0.x/\n",
    "- Miguel Grinberg's Flask Mega-Tutorial: https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world\n",
    "- Corey Schafer's Flask Tutorial Series: https://www.youtube.com/playlist?list=PL-osiE80TeTs4UjLw5MM6OjgkjFeUxCYH\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tīmekļa lapu rasmošana - skrāpēšana - kas tas ir?\n",
    "\n",
    "Tīmekļa lapu rasmošana ir datu iegūšanas process no tīmekļa lapām. Tas ietver HTTP pieprasījumu nosūtīšanu uz tīmekļa lapu, HTML satura parsēšanu un nepieciešamo datu iegūšanu. Tīmekļa lapu rasmošana tiek izmantota datu iegūšanai, cenu uzraudzībai un satura apkopošanai un citiem mērķiem.\n",
    "\n",
    "Teorētiski iespējams rasmošanu veikt arī manuāli, vienkārši saglabājot apmeklētās tīmekļa lapas saturu, bet bieži vien ir efektīvāk izmantot tīmekļa rasmošanas bibliotēku, piemēram, Scrapy, lai automatizētu procesu.\n",
    "\n",
    "### Rasmošanas noteikumi\n",
    "\n",
    "* Spēlējiet tīri! Neveidojiet pārlieku slodzi tīmekļa vietnei ar pieprasījumiem, jo tas var izraisīt servera problēmas un jūs varat tikt nobloķēts.\n",
    "* Pirms sākat rasmošanu, pārbaudiet tīmekļa vietnes lietošanas noteikumus un robots.txt failu, lai pārliecinātos, ka neaizskarāt kādus noteikumus.\n",
    "* Ja iespējams iegūt datus caur API, ir ieteicams izmantot API, nevis rasmošanu.\n",
    "* Iegūtās datus izmantojiet tikai saskaņā ar autortiesībām un likumiem.\n",
    "* Ja iespējams, izmantojiet jau publisku datukopu - vislabāk no pašiem lietotnes autoriem, nevis tīmekļa lapas rasmošanu.\n",
    "* Izmantojiet tīmekļa lapu rasmošanu atbildīgi un etiski.\n",
    "* Ievērojiet atšķirību starp kādu datu rasmošanu un to publicēšanu. Rasmošana ir tikai datu iegūšana, bet publicēšana ir atsevišķs jautājums.\n",
    "* Pētnieciskiem mērķiem ir ieteicams iegūt atļauju no tīmekļa vietnes īpašniekiem, ja iespējams.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy - Python tīmekļa rasmošanas bibliotēka\n",
    "\n",
    "[Scrapy](https://scrapy.org) ir jaudīga tīmekļa rasmošanas bibliotēka Python, kas padara vieglu datu iegūšanu no tīmekļa vietnēm. Tā nodrošina augsta līmeņa API tīmekļa vietņu pārlūkošanai un datu iegūšanai, padarot to par lielisku izvēli tīmekļa rasmošanas projektos.\n",
    "\n",
    "### Alternative to Scrapy - BeautifulSoup\n",
    "\n",
    "Beautiful Soup ir vēl viena populāra tīmekļa rasmošanas bibliotēka Python. Tā ir viegla bibliotēka, kas ir viegli lietojama un lieliska vienkāršiem tīmekļa rasmošanas uzdevumiem. Tomēr, ja jums ir nepieciešamas papildu funkcijas, piemēram, vairāku lapu pārlūkošana, sīkdatņu un sesiju apstrāde un datu iegūšana no sarežģītām tīmekļa vietnēm, Scrapy ir labāka izvēle.\n",
    "\n",
    "Vairāk par BeautifulSoup varat uzzināt šeit: https://www.crummy.com/software/BeautifulSoup/\n",
    "\n",
    "### Kā strādā Scrapy\n",
    "\n",
    "Scrapy strādā, nosūtot HTTP pieprasījumus uz tīmekļa vietni, parsējot HTML saturu un izmantojot selektoru sistēmu, lai iegūtu nepieciešamos datus. Tas nodrošina vairākas funkcijas, kas padara tīmekļa rasmošanu vieglu un efektīvu, piemēram, iebūvēto tīmekļa pārlūkošanu, spēcīgu selektoru sistēmu un atbalstu sīkdatnēm un sesijām.\n",
    "\n",
    "### Scrapy uzstādīšana\n",
    "\n",
    "Kā parasti, ieteicams vispirms izveidot un aktivizēt virtuālo vidi pirms instalējat Scrapy. Skat iepr. Flask uzstādīšanas instrukcijas.\n",
    "\n",
    "Srapy var instalēt ar `pip`:\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install Scrapy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vienkāršs tīmekļa rasmošanas piemērs ar Scrapy\t\n",
    "\n",
    "Pieņemsim mēs vēlamies iegūt pilsētas un to iedzīvotāju skaitu no Vikipēdijas lapas: https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\n",
    "(Lūdzu ņemiet vērā, ka Vikipēdija piedāvā [API](https://www.mediawiki.org/wiki/API:Main_page), lai piekļūtu saviem datiem, tāpēc rasmošana nav nepieciešama šajā gadījumā. Tas ir tikai mācību piemērs.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy version is 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# first let's import scrapy and check its version\n",
    "try:\n",
    "    import scrapy\n",
    "    print(f\"Scrapy version is {scrapy.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Scrapy is not installed\")\n",
    "    print(\"You can install Scrapy with pip install scrapy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 19:38:03 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): en.wikipedia.org:443\n",
      "2024-12-09 19:38:03 [urllib3.connectionpool] DEBUG: https://en.wikipedia.org:443 \"GET /wiki/List_of_cities_in_Latvia HTTP/11\" 200 38490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will scrape https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\n",
      "Web page title is List of cities and towns in Latvia - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "# now let's scrape the following web page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_cities_in_Latvia'\n",
    "print(f\"We will scrape {url}\")\n",
    "# Note: usually there is no need to scrape Wikipedia as they have APIs and data dumps available\n",
    "# but for learning purposes we will scrape this page\n",
    "# we are interested in table with cities and their population\n",
    "\n",
    "# we've already imported scrapy so we can start using it\n",
    "# let's start with basic example where we simply fetch the page and extract the title\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "try:\n",
    "    import requests  # we also use requests library to fetch the page\n",
    "except ImportError:\n",
    "    print(\"Requests is not installed\")\n",
    "    print(\"You can install Requests with pip install requests\")\n",
    "\n",
    "# NOTE: Scrapy has its own request object as well, but it is more complex\n",
    "\n",
    "response = requests.get(url)\n",
    "# let's create a scrapy response object\n",
    "scrapy_response = HtmlResponse(url, body=response.text, encoding='utf-8')\n",
    "\n",
    "# let's extract the title using css selector\n",
    "title = scrapy_response.css('title::text').get()\n",
    "print(\"Web page title is\", title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS selektoru izmantošana datu iegūšanai\n",
    "\n",
    "Scrapy piedāvā spēcīgu selektoru sistēmu, kas ļauj jums izgūt datus no HTML, izmantojot CSS selektorus. Jūs varat izmantot CSS selektorus, lai mērķētu konkrētus elementus lapā un iegūtu nepieciešamos datus.\n",
    "\n",
    "Lai izmantotu CSS selektorus Scrapy, jums būs jāzina CSS selektoru sintakse. Šeit ir daži pamata piemēri:\n",
    "\n",
    "#### Pamata CSS piemēri:\n",
    "\n",
    "| CSS selektors | Apraksts |\n",
    "| --- | --- |\n",
    "| tag | Izvēlas visus <tag> elementus. |\n",
    "| .class | Izvēlas visus elementus ar klasi class. |\n",
    "| #id | Izvēlas elementu ar ID id. |\n",
    "| tag.class | Izvēlas <tag> elementus ar klasi class. |\n",
    "| tag[attr=\"value\"] | Izvēlas <tag> elementus ar atribūtu attr=\"value\". |\n",
    "| tag > child | Izvēlas <tag> elementa tiešos bērnus. |\n",
    "| tag child | Izvēlas visus <tag> elementa pēcnācējus. |\n",
    "| tag:first-child | Izvēlas pirmo <tag> bērnu. |\n",
    "| tag:nth-child(n) | Izvēlas nth <tag> bērnu. |\n",
    "\n",
    "#### Scrapy CSS metodes: \n",
    "\n",
    "- `response.css('<CSS selector>')`: Izgūst elementus, kas atbilst CSS selektoram.\n",
    "- `.get()`: Atgriež pirmo atbilstošo vērtību.\n",
    "- `.getall()`: Atgriež visus atbilstošos vērtības sarakstā.\n",
    "- `.re('<regex>')`: Izgūst vērtības, kas atbilst regulārai izteiksmē.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 tables on the page\n",
      "There are 2 wikitable sortable tables on the page\n",
      "Table(s) have rows 83\n"
     ]
    }
   ],
   "source": [
    "# now let's use Selector to get the table with cities and their population\n",
    "# we will use CSS selectors\n",
    "# we will use the table with class wikitable sortable\n",
    "\n",
    "# first let's see about getting all tables\n",
    "tables = scrapy_response.css('table')\n",
    "# how many tables are there?\n",
    "print(\"There are\", len(tables), \"tables on the page\")\n",
    "\n",
    "# by using inspect element in browser we can see that the table we want is the first table with class wikitable sortable\n",
    "table = scrapy_response.css('table.wikitable.sortable') \n",
    "# how many tables are there?\n",
    "print(\"There are\", len(table), \"wikitable sortable tables on the page\")\n",
    "# in our case this is fine as we want elements from both tables\n",
    "# if we only needed the first table we could use table = scrapy_response.css('table.wikitable.sortable')[0] as the first table is at index 0\n",
    "\n",
    "# how many tr elements are in the tables\n",
    "rows = table.css('tr')\n",
    "print(\"Table(s) have rows\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest five cities [{'city': ['Rīga', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '658,640\\n'}, {'city': ['Daugavpils', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '93,312\\n'}, {'city': ['Liepāja', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '76,731\\n'}, {'city': ['Jelgava', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '59,511\\n'}, {'city': ['Jūrmala', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '50,840\\n'}]\n",
      "Smallest five cities [{'city': ['Varakļāni', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '2,106\\n'}, {'city': ['Viesīte', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '1,902\\n'}, {'city': ['Viļaka', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '1,525\\n'}, {'city': ['Viļāni', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '3,468\\n'}, {'city': ['Zilupe', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '1,746\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# let's save data in a list of dictionaries for easier processing\n",
    "cities = []\n",
    "# let's iterate over rows and extract the data\n",
    "for row in rows:\n",
    "    # city = row.css('td:nth-child(1)::text').get()\n",
    "    # we want all text from first child td element that is contained in its children\n",
    "    city = row.css('td:nth-child(1) *::text').getall() # note the * after td:nth-child(1) we get all text from all children\n",
    "    population = row.css('td:nth-child(2)::text').get() # here we get only the text from the td element\n",
    "    if city and population: # we only want rows with city and population\n",
    "        cities.append({'city': city, 'population': population})\n",
    "# print first 5 cities\n",
    "print(\"Biggest five cities\", cities[:5])\n",
    "# last 5 cities\n",
    "print(\"Smallest five cities\", cities[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest five cities [{'city': 'Rīga', 'population': '658,640\\n'}, {'city': 'Daugavpils', 'population': '93,312\\n'}, {'city': 'Liepāja', 'population': '76,731\\n'}, {'city': 'Jelgava', 'population': '59,511\\n'}, {'city': 'Jūrmala', 'population': '50,840\\n'}]\n",
      "Smallest five cities [{'city': 'Varakļāni', 'population': '2,106\\n'}, {'city': 'Viesīte', 'population': '1,902\\n'}, {'city': 'Viļaka', 'population': '1,525\\n'}, {'city': 'Viļāni', 'population': '3,468\\n'}, {'city': 'Zilupe', 'population': '1,746\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# we can see that we only care about the first entry for city name so let's clean up the list of dictionaries\n",
    "# we will keep only the first entry for city name\n",
    "cities = [{'city': d['city'][0], 'population': d['population']} for d in cities] # list comprehension\n",
    "# print first 5 cities\n",
    "print(\"Biggest five cities\", cities[:5])\n",
    "# last 5 cities\n",
    "print(\"Smallest five cities\", cities[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we could save the results to a file or database\n",
    "# alternatively we could load the data into a pandas DataFrame for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XPath selektoru izmantošana datu iegūšanai\n",
    "\n",
    "XPath ir jaudīgāka alternatīva CSS selektoriem, kas ļauj jums veikt sarežģītākus datu iegūšanas uzdevumus. XPath ļauj jums navigēt un vaicāt elementus HTML vai XML dokumentā. Scrapy `response.xpath()` metode nodrošina saskarni elementu atlasīšanai, pamatojoties uz XPath izteiksmēm.\n",
    "\n",
    "Lai izmantotu XPath Scrapy, jums būs jāzina XPath sintakse. Šeit ir daži pamata piemēri:\n",
    "\n",
    "#### Pamata XPath piemēri:\n",
    "\n",
    "| XPath izteiksmes | Apraksts |\n",
    "| --- | --- |\n",
    "| //tag | Izvēlas visus <tag> elementus jebkur, kur tie atrodas dokumentā. |\n",
    "| ./tag | Izvēlas visus <tag> elementus tieši zem pašreizējā elementa. |\n",
    "| //tag[@attr=\"value\"] | Izvēlas <tag> elementus ar atribūtu attr=\"value\". |\n",
    "| //tag/text() | Izvēlas <tag> elementu teksta saturu. |\n",
    "| //tag[contains(@attr, \"val\")] | Izvēlas <tag> elementus, kur attr satur \"val\". |\n",
    "| //tag[1] | Izvēlas pirmo <tag> elementu kontekstā. |\n",
    "| //tag[last()] | Izvēlas pēdējo <tag> elementu kontekstā. |\n",
    "| //tag[position() < 3] | Izvēlas pirmos divus <tag> elementus. |\n",
    "\n",
    "#### Scrapy XPath metodes:\n",
    "\n",
    "- `response.xpath('<XPath>')`: Izgūst elementus, kas atbilst XPath izteiksmēm.\n",
    "- `.get()`: Atgriež pirmo atbilstošo vērtību.\n",
    "- `.getall()`: Atgriež visus atbilstošos vērtības sarakstā.\n",
    "- `.extract()`: Aliass `.getall()`, bet ir novecojis.\n",
    "- `.re('<regex>')`: Izgūst vērtības, kas atbilst regulārai izteiksmē.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 wikitable sortable tables on the page\n"
     ]
    }
   ],
   "source": [
    "# now let's see how we could have used XPath selectors to get the same data\n",
    "\n",
    "# let's extract the table with XPath\n",
    "table = scrapy_response.xpath('//table[contains(@class, \"wikitable\") and contains(@class, \"sortable\")]')\n",
    "# how many tables are there?\n",
    "print(\"There are\", len(table), \"wikitable sortable tables on the page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table(s) have rows 83\n"
     ]
    }
   ],
   "source": [
    "# now let's extract the rows with XPath\n",
    "rows = table.xpath('.//tr')\n",
    "print(\"Table(s) have rows\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try something slightly fancier we will want to extract text from first anchor child in first td element - city name\n",
    "# example\n",
    "# <tr>\n",
    "# <td><a href=\"/wiki/R%C4%ABga\" class=\"mw-redirect\" title=\"Rīga\">Rīga</a>&nbsp;<small><span class=\"noprint\"><span class=\"ext-phonos\"><span data-nosnippet=\"\" id=\"ooui-php-1\" class=\"ext-phonos-PhonosButton noexcerpt oo-ui-widget oo-ui-widget-enabled oo-ui-buttonElement oo-ui-buttonElement-frameless oo-ui-iconElement oo-ui-labelElement oo-ui-buttonWidget\" data-ooui=\"{&quot;_&quot;:&quot;mw.Phonos.PhonosButton&quot;,&quot;href&quot;:&quot;\\/\\/upload.wikimedia.org\\/wikipedia\\/commons\\/transcoded\\/f\\/f5\\/Lv-R%C4%ABga.ogg\\/Lv-R%C4%ABga.ogg.mp3&quot;,&quot;rel&quot;:[&quot;nofollow&quot;],&quot;framed&quot;:false,&quot;icon&quot;:&quot;volumeUp&quot;,&quot;label&quot;:{&quot;html&quot;:&quot;pronunciation&quot;},&quot;data&quot;:{&quot;ipa&quot;:&quot;&quot;,&quot;text&quot;:&quot;&quot;,&quot;lang&quot;:&quot;en&quot;,&quot;wikibase&quot;:&quot;&quot;,&quot;file&quot;:&quot;Lv-R\\u012bga.ogg&quot;},&quot;classes&quot;:[&quot;ext-phonos-PhonosButton&quot;,&quot;noexcerpt&quot;]}\"><a role=\"button\" tabindex=\"0\" href=\"//upload.wikimedia.org/wikipedia/commons/transcoded/f/f5/Lv-R%C4%ABga.ogg/Lv-R%C4%ABga.ogg.mp3\" rel=\"nofollow\" aria-label=\"Play audio\" title=\"Play audio\" class=\"oo-ui-buttonElement-button\"><span class=\"oo-ui-iconElement-icon oo-ui-icon-volumeUp\"></span><span class=\"oo-ui-labelElement-label\">pronunciation</span><span class=\"oo-ui-indicatorElement-indicator oo-ui-indicatorElement-noIndicator\"></span></a></span><sup class=\"ext-phonos-attribution noexcerpt navigation-not-searchable\"><a href=\"/wiki/File:Lv-R%C4%ABga.ogg\" title=\"File:Lv-Rīga.ogg\">ⓘ</a></sup></span></span></small>&nbsp;<figure class=\"mw-halign-right\" typeof=\"mw:File\"><a href=\"/wiki/File:Greater_Coat_of_Arms_of_Riga_-_for_display.svg\" class=\"mw-file-description\"><img src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Greater_Coat_of_Arms_of_Riga_-_for_display.svg/55px-Greater_Coat_of_Arms_of_Riga_-_for_display.svg.png\" decoding=\"async\" width=\"55\" height=\"33\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Greater_Coat_of_Arms_of_Riga_-_for_display.svg/83px-Greater_Coat_of_Arms_of_Riga_-_for_display.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/99/Greater_Coat_of_Arms_of_Riga_-_for_display.svg/110px-Greater_Coat_of_Arms_of_Riga_-_for_display.svg.png 2x\" data-file-width=\"510\" data-file-height=\"303\"></a><figcaption></figcaption></figure>\n",
    "# </td>\n",
    "# <td>658,640\n",
    "# </td>\n",
    "# <td>632,614\n",
    "# </td>\n",
    "# <td>605,273\n",
    "# </td></tr>\n",
    "\n",
    "# here we can see that city name Rīga is in the first td element inside its first anchor child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest five cities [{'city': 'Rīga', 'population': '658,640\\n'}, {'city': 'Daugavpils', 'population': '93,312\\n'}, {'city': 'Liepāja', 'population': '76,731\\n'}, {'city': 'Jelgava', 'population': '59,511\\n'}, {'city': 'Jūrmala', 'population': '50,840\\n'}]\n",
      "Smallest five cities [{'city': 'Varakļāni', 'population': '2,106\\n'}, {'city': 'Viesīte', 'population': '1,902\\n'}, {'city': 'Viļaka', 'population': '1,525\\n'}, {'city': 'Viļāni', 'population': '3,468\\n'}, {'city': 'Zilupe', 'population': '1,746\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# now let's save data in a list of dictionaries for easier processing\n",
    "also_cities = []\n",
    "# let's iterate over rows and extract the data\n",
    "for row in rows:\n",
    "    # city = row.xpath('.//td[1]//text()').getall() # we want all text from first child td element that is contained in its children\n",
    "    # we want text from first anchor element in td[1]\n",
    "    city = row.xpath('.//td[1]//a[1]//text()').get() # note the //a[1] we get text from first anchor child of the first td element\n",
    "    population = row.xpath('.//td[2]//text()').get() # here we get only the text from the td element\n",
    "    if city and population: # we only want rows with city and population\n",
    "        also_cities.append({'city': city, 'population': population})\n",
    "# print first 5 cities\n",
    "print(\"Biggest five cities\", also_cities[:5])\n",
    "# last 5 cities\n",
    "print(\"Smallest five cities\", also_cities[-5:])\n",
    "\n",
    "# using XPath we did not have to do any Python list comprehension to clean up the data after extraction\n",
    "# now we could still clean up newlines and convert population to integer but that is easy and not part of scraping itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atšķirība starp CSS un XPath pieprasījumiem Scrapy\n",
    "\n",
    "| Iespēja | XPath | CSS |\n",
    "| --- | --- | --- |\n",
    "| Sintakse | Izteiksmīgāka un sarežģīta | Vienkārša un viegli lasāma |\n",
    "| Veiktspēja | Parasti lēnāka | Parasti ātrāka |\n",
    "| Teksta saturu | Jāizmanto `text()` funkcija | Izmanto `::text` seletoru |\n",
    "| Atribūtu atbilstība | Izmanto `@attr=\"value\"` | Izmanto `[attr=\"value\"]` seletoru |\n",
    "| Pozicionālā atbilstība | Izmanto `position()=n` | Izmanto `:nth-child(n)` seletoru |\n",
    "| Navigācija | Atbalsta vecāku/brāļu navigāciju | Ierobežots līdz bērnu navigācijai |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vairāku lapu rasmošana ar Scrapy\n",
    "\n",
    "Vairāku lapu rasmošana ir bieži sastopams uzdevums, kad jums ir nepieciešams iegūt datus no vairākām lapām. Scrapy piedāvā iespēju sekot saitēm un iegūt datus no vairākām lapām, izmantojot tāja iebūvēto tīmekļa pārlūkošanas funkciju.\t\n",
    "\n",
    "Pagaidām mēs izmantojam iepriekš noteiktu URL sarakstu, lai rasmotu, bet reālā dzīvē jūs, iespējams, vēlēsieties rasmot saites no kādas lapas/lapām pašām.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will scrape the following pages:\n",
      "https://en.wikipedia.org/wiki/List_of_cities_in_Estonia\n",
      "https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\n",
      "https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania\n"
     ]
    }
   ],
   "source": [
    "start_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Estonia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania\",\n",
    "]\n",
    "print(\"We will scrape the following pages:\", *start_urls, sep=\"\\n\") # we using * to unpack the list into separate arguments for neat printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:14:36 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: scrapybot)\n",
      "2024-12-09 20:14:36 [scrapy.utils.log] INFO: Versions: lxml 5.2.2.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.11.0, Python 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)], pyOpenSSL 24.3.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Windows-11-10.0.22631-SP0\n",
      "2024-12-09 20:14:36 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-12-09 20:14:36 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-12-09 20:14:36 [scrapy.extensions.telnet] INFO: Telnet Password: 230407da82ccaf53\n",
      "2024-12-09 20:14:36 [py.warnings] WARNING: c:\\pyenvs\\venv312\\Lib\\site-packages\\scrapy\\extensions\\feedexport.py:432: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n",
      "2024-12-09 20:14:36 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-12-09 20:14:36 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-12-09 20:14:37 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-12-09 20:14:37 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-12-09 20:14:37 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-12-09 20:14:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-12-09 20:14:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_cities_in_Estonia> (referer: None)\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania> (referer: None)\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_cities_in_Latvia> (referer: None)\n",
      "2024-12-09 20:14:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/List_of_cities_in_Estonia>\n",
      "{'title': 'List of cities and towns in Estonia - Wikipedia'}\n",
      "2024-12-09 20:14:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania>\n",
      "{'title': 'List of cities in Lithuania - Wikipedia'}\n",
      "2024-12-09 20:14:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/List_of_cities_in_Latvia>\n",
      "{'title': 'List of cities and towns in Latvia - Wikipedia'}\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-12-09 20:14:37 [scrapy.extensions.feedexport] INFO: Stored json feed (3 items) in: cities_titles.json\n",
      "2024-12-09 20:14:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 760,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 119110,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 3,\n",
      " 'elapsed_time_seconds': 0.306788,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 12, 9, 18, 14, 37, 551379, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 819985,\n",
      " 'httpcompression/response_count': 3,\n",
      " 'item_scraped_count': 3,\n",
      " 'items_per_minute': None,\n",
      " 'log_count/DEBUG': 7,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 3,\n",
      " 'responses_per_minute': None,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2024, 12, 9, 18, 14, 37, 244591, tzinfo=datetime.timezone.utc)}\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# we have the start_urls now we can create a Scrapy spider to scrape these pages\n",
    "# in this case we will use a simple spider that will only extract the title of the page\n",
    "\n",
    "# let's do a simple spider that will extract the title of the page\n",
    "\n",
    "# we want to save these titles in a list of dictionaries as JSON\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class SimpleSpider(scrapy.Spider):\n",
    "    name = 'simple_spider'\n",
    "    start_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Estonia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania\",\n",
    "] # we could have passed this as a parameter to the spider\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').get()\n",
    "        # ADD your own code here to extract more data from each page\n",
    "        # you can use either css or xpath selectors\n",
    "        yield {'title': title}\n",
    "\n",
    "# let's run the spider\n",
    "# we also want to save the results to a file\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    'FEED_URI': 'cities_titles.json',\n",
    "    'FEED_FORMAT': 'json'\n",
    "})\n",
    "\n",
    "process.crawl(SimpleSpider)\n",
    "process.start()\n",
    "# NOTE this process is really meant to be run from a script or terminal\n",
    "# if you run it in a Jupyter notebook you might have to restart the kernel to run it again\n",
    "# otherwise you will get errors about already running Twisted reactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vairāku lapu rasmošanas piemērs ar Scrapy\n",
    "\n",
    "1. **Izveidojiet zirnekli - Spider** : \n",
    "  - Iekļaujiet visas URL sarakstā `start_urls` vai ielādējiet tās dinamiski, izmantojot `start_requests()`.\n",
    "\n",
    "2. **Parsējiet katru lapu** :\n",
    "  - Izmantojiet to pašu `parse()` metodi datu izgūšanai, jo lapu struktūra ir līdzīga.\n",
    " \n",
    "3. **Identificējiet kontekstu** :\n",
    "  - Izmantojiet informāciju no URL vai lapas satura, lai iezīmētu datus (piemēram, valsts nosaukumu).\n",
    "\n",
    "4. **Palaižiet zirnekli** :\n",
    "  - Palaidiet zirnekli un iespējams saglabājiet rezultātus failā. \n",
    "\n",
    "5. **Pēcapstrāde** :\n",
    "  - Pārbaudiet un attīrīet datus pēc nepieciešamības (piem. apvienojiet JSON failus, CSV tabulas).\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy Learning References\n",
    "\n",
    "For further exploration of Scrapy, you can refer to the following resources:\n",
    "\n",
    "- Official Scrapy Documentation: https://docs.scrapy.org/en/latest/\n",
    "- Scrapy Tutorial: https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    "- YouTube Tutorial: https://www.youtube.com/watch?v=ve_0h4Y8nuI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "### Flask Practice\n",
    "\n",
    "1. Create a simple web application using Flask that displays a list of items. The list should be stored in a Python list and displayed on the web page.\n",
    "2. Add a form to the web application that allows users to add new items to the list.\n",
    "3. Add a delete button next to each item in the list that allows users to delete items from the list.\n",
    "\n",
    "### Scrapy Practice\n",
    "\n",
    "1. Create a Scrapy spider that scrapes data from a website of your choice. The spider should extract at least two fields from the website and save the data to a JSON or CSV file.\n",
    "2. Modify the spider to save the data to a database instead of a JSON or CSV file.\n",
    "3. Add error handling to the spider to handle cases where the website is down or the data is missing.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, we covered the basics of server-side web development using Flask and web scraping using Scrapy. We learned how to create a simple web application with Flask and how to scrape data from a website using Scrapy. We also discussed the rules of web scraping and best practices for working with web scraping libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
