{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LU Logo](https://www.lu.lv/fileadmin/user_upload/LU.LV/www.lu.lv/Logo/Logo_jaunie/LU_logo_LV_horiz.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tīmekļa lapu izstrāde ar Flask un datu iegūšana(rasmošana) ar Scrapy\n",
    "\n",
    "## Nodarbības saturs\n",
    "\n",
    "Mēs apskatīsim šādas tēmas šajā nodarbībā:\n",
    "\n",
    "* Flask - Python servera puses tīmekļa izstrādes rīks jeb sastatnes\n",
    "* Scrapy - Python bibliotēka datu iegūšanai no tīmekļa lapām, jeb rasmošanai\n",
    "\n",
    "## Nodarbības mērķi\n",
    "\n",
    "Nodarbības beigās jūs būsiet spējīgi:\n",
    "\n",
    "* Saprast servera puses tīmekļa izstrādes pamatus\n",
    "* Izveidot vienkāršu tīmekļa lietotni, izmantojot Flask\n",
    "* Saprast tīmekļa lapu rasmošanas pamatus\n",
    "* Iegūt datus no tīmekļa lapas, izmantojot Scrapy\n",
    "\n",
    "## Nepieciešamās priekšzināšanas\n",
    "\n",
    "Pirms sākat šo nodarbību, jums vajadzētu:\n",
    "\n",
    "* Saprast Python programmēšanas pamatus - mainīgie, datu tipi, kontroles struktūras, funkcijas, OOP un failu ievade/izvade\n",
    "* Zināt, kā instalēt Python pakotnes, izmantojot `pip`\n",
    "* Zināt, kā izveidot virtuālo vidi, izmantojot `venv`\n",
    "* Saprast HTML pamatus - skatiet MDN Web Docs [HTML ievads](https://developer.mozilla.org/en-US/docs/Learn/HTML/Introduction_to_HTML)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tīmekļa lapu izstrādes pamati\n",
    "\n",
    "Tīmekļa lapu izstrāde ir plašs jēdziens, kas ietver daudzas dažādas tehnoloģijas un prasmes.\n",
    "\n",
    "Viens dalījums būtu pēc tā, vai jūs strādājat ar lietotāja saskarni (front end) vai ar servera pusi (back end).\n",
    "\n",
    "\n",
    "### Lietotājā saskarne - Front End Web Development\n",
    "\n",
    "Lietotāja saskarnes izstrāde ietver tīmekļa lapas lietotāja saskarnes un lietotāja pieredzes(UX) izveidi. Tas ietver tīmekļa lapas izkārtojuma, krāsu, fontu un interaktīvo elementu projektēšanu. Front end izstrādātāji izmanto HTML, CSS un JavaScript, lai izveidotu tīmekļa lapas vizuālos elementus, ar kuriem lietotāji mijiedarbojas.\n",
    "\n",
    "\n",
    "### Servera izstrāde - Back End Web Development\n",
    "\n",
    "Servera puses tīmekļa izstrāde ietver servera puses loģikas un datu bāzes mijiedarbības izveidi. Tas ietver lietotāja pieprasījumu apstrādi, datu apstrādi un dinamiskas satura ģenerēšanu. Servera puses izstrādātāji izmanto servera puses programmēšanas valodas, piemēram, Python, PHP, Ruby, Java un citas, lai izveidotu tīmekļa lapas servera puses komponentus.\n",
    "\n",
    "API izstrāde, lietotāja autentifikācijas apstrāde un datu bāzu pārvaldība ir daži no uzdevumiem, par kuriem atbild back end izstrādātāji.\n",
    "\n",
    "### Pilna tīmekļa izstrāde - Full Stack Web Development\n",
    "\n",
    "Pilna tīmekļa izstrāde ietver darbu gan ar front end, gan ar back end tehnoloģijām. Pilna tīmekļa izstrādātāji ir prasmīgi gan front end, gan back end tehnoloģijās un var izveidot pilnīgas tīmekļa lietotnes no sākuma līdz beigām.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask - Python tīmekļa izstrādes rīks\n",
    "\n",
    "Flask ir vienkāršs un viegli lietojams Python tīmekļa izstrādes rīks. Tas ir paredzēts, lai palīdzētu jums izveidot tīmekļa lietotnes ātri un vienkārši.\n",
    "To izmanto gan pieredzējuši izstrādātāji, gan iesācēji, jo tas ir viegli saprotams un lietojams.\n",
    "\n",
    "### Kā darbojas Flask\n",
    "\n",
    "\n",
    "Flask darbojas, izmantojot WSGI (Web Server Gateway Interface) standartu, kas ļauj to darboties ar dažādiem tīmekļa serveriem. Tas nozīmē, ka jūs varat izmantot Flask ar dažādiem tīmekļa serveriem, piemēram, Apache, Nginx vai citiem.\n",
    "\n",
    "Flask izmanto dekoratorus, lai definētu maršrutus un funkcijas, kas tiek izpildītas, kad tiek saņemts pieprasījums uz konkrēto maršrutu. Tas ļauj jums viegli definēt, kāda darbība jāveic, kad tiek saņemts pieprasījums uz konkrēto URL. Tos apskatīsim vēlāk šajā nodarbībā."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtuālās vides iestatīšana\n",
    "\n",
    "Pirms instalējat Flask, IZSTRĀDĀTĀJIEM ĻOTI IESAKĀMS izveidot virtuālo vidi savam projektam. Tas palīdzēs jums pārvaldīt atkarības un izvairīties no konfliktiem ar citiem projektiem.\n",
    "\n",
    "Pastāv vairāki veidi, kā izveidot virtuālo vidi Python. Viena no visbiežāk izmantotajām metodēm ir izmantot iebūvēto `venv` moduli. Šādi varat izveidot virtuālo vidi, izmantojot `venv`:\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create a new directory for your project\n",
    "mkdir myproject\n",
    "cd myproject\n",
    "python -m venv myvenv\n",
    "```\n",
    "\n",
    "Tā vietā, lai izmantotu myenv, varat izmantot jebkuru vēlamo nosaukumu savai virtuālajai videi. \n",
    "\n",
    "#### Virtuālās vides aktivizēšana\n",
    "\n",
    "Lai aktivizētu virtuālo vidi, varat izmantot šādu komandu:\n",
    "\n",
    "```bash\n",
    "# On Windows\n",
    "myvenv\\Scripts\\activate\n",
    "\n",
    "# On macOS and Linux\n",
    "source myvenv/bin/activate\n",
    "```\n",
    "\n",
    "### Flask uzstādīšana\n",
    "\n",
    "Kad esat izveidojis un AKTIVIZĒJIS savu virtuālo vidi, varat instalēt Flask, izmantojot `pip`:\n",
    "\n",
    "```bash\n",
    "\n",
    "pip install Flask\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veidojam vienkāršu tīmekļa lietotni ar Flask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello Pasaule piemērs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "# izveidosim vienkāršu Flask aplikāciju\n",
    "\n",
    "from flask import Flask # import the Flask class from the flask module\n",
    "\n",
    "app = Flask(__name__) # this creates a new Flask app object\n",
    "\n",
    "# we will be using app.route() decorator to define the URL that will trigger the function below\n",
    "@app.route('/') # this route means that the function below will be called when the user goes to the root URL of your website\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "# you'd add this line to run the app in script mode\n",
    "# if __name__ == '__main__':\n",
    "#     app.run()\n",
    "\n",
    "app.run() # this is the same as the above line, but it's not recommended to use this in script mode\n",
    "# usually you would not run this from Jupiter notebook, but from a terminal\n",
    "\n",
    "# use Ctrl+C to stop the server on terminal\n",
    "# in Jupyter notebook, you can stop the server by clicking on the stop button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametru izmantošana maršrutos\n",
    "\n",
    "Nākošais solis ir izveidot vienkāršu tīmekļa lietotni, kas ņem parametru URL un parāda to lapā. Šeit ir piemērs:\n",
    "\n",
    "\n",
    "```python\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/greet/<name>')\n",
    "def greet(name):\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Dec/2024 16:54:53] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 16:54:59] \"GET /greet/Valdis HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 16:55:05] \"GET /greet/LUPython HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 16:55:11] \"GET /greet/LUPython/Latvia HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "# note that the URL is case-sensitive\n",
    "# note the use of <name> in the URL this is a variable part of the URL\n",
    "@app.route('/greet/<name>')\n",
    "def greet(name):\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "# note that only first level of the URL will be caught by this route\n",
    "# so /greet/Janis/Berzins will not work - you would need a separate route for that\n",
    "# but /greet/Janis will work\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n",
    "# use Ctrl+C to stop the server on terminal\n",
    "# in Jupyter notebook, you can stop the server by clicking on the stop button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pieprasījumu parametru izmantošana\n",
    "\n",
    "Pieprasījuma parametri ir vēl viens veids, kā padot datus tīmekļa lietotnei. Tos pievieno URL pēc jautājuma zīmes `?` un ir formā `key=value`. Šeit ir piemērs:\n",
    "\n",
    "```python\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/sveiks')\n",
    "def greet():\n",
    "    name = request.args.get('name', 'World')\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "```\n",
    "\n",
    "Tagad varat piekļūt sveiks maršrutam ar pieprasījuma parametru šādi: `http://localhost:5000/sveiks?name=Uldis`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Dec/2024 17:07:54] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 17:08:03] \"GET /sveiks?name=Valdis HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "## Using query parameters\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/sveiks')\n",
    "def greet():\n",
    "    name = request.args.get('name', 'Pasaule') #if no name argument is given, we will use 'Pasaule'\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n",
    "# on local server try something like http://127.0.0.1:5000/sveiks?name=Valdis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Šablonu un statisko failu izmantošana\n",
    "\n",
    "Parasti mēs nevēlamies apstrādāt HTML savā Python kodā. Flask ļauj izmantot šablonus, lai atdalītu HTML no Python koda. Flask izmanto Jinja2 kā savu šablonu dzinēju.\n",
    "\n",
    "Pilna dokumentācija par Jinja2 atrodama [šeit](https://jinja.palletsprojects.com/en/3.0.x/)\n",
    "\n",
    "Pamatideja ir tāda, ka izveidojat mapi `templates` savam projektam un ievietojat HTML failus tur. Ievērosim ka šabloni ļaus mums ievietot mainīgos un izteiksmes, kas tiks aizstātas ar reāliem datiem, kad šablons tiks renderēts.\n",
    "\n",
    "Papildus dinamiskam saturam, jums var būt nepieciešami arī statiskie faili, piemēram, CSS, JavaScript, attēli utt. Lai tos iekļautu savā projektā, izveidojiet mapi `static` un ievietojiet failus tur.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Dec/2024 18:24:57] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:24:57] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:24:57] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:00] \"GET /static/mystyle.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:12] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:12] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:12] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:13] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:13] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:13] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:14] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:15] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:15] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:15] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:17] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:17] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:17] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:18] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:18] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:18] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:21] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:21] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:21] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /about HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /static/myscript.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /static/mystyle.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 18:25:24] \"GET /static/myscript.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# example of using templates and static files\n",
    "from flask import Flask, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    items = [\"Saraksti\", \"Vārdnīcas\", \"Citi objekti\"] # these could be from a database or other data source\n",
    "    # also these objects could be from url query parameters\n",
    "    return render_template('index.html', year=2024, items=items) # thus template will receive year and items\n",
    "# see index.html templates folder on how it is handled on the template side\n",
    "# you might also check out base.html to see how templates can be extended\n",
    "\n",
    "@app.route('/about')\n",
    "def about():\n",
    "    return render_template('about.html', year=2024)\n",
    "# about.html is even simpler template than index.html\n",
    "# it also extends base.html in templates folder\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # app.run(debug=True) # debug mode will reload the server on code changes and provide more verbose output\n",
    "    app.run() # debug mode will reload the server on code changes and provide more verbose output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask projektu struktūra\n",
    "\n",
    "Lai jūsu Flask projekts būtu labi organizēts, jūs varat izmantot šādu struktūru:\n",
    "\n",
    "```\n",
    "project_root/\n",
    "│\n",
    "├── app.py                # Jūsu galvenais Flask fails - iespējams, ka būs arī citi .py faili\n",
    "├── static/               # Mape statiskiem failiem( CSS, JavaScript, attēli, utt.)\n",
    "│   └── styles.css        # Jūsu CSS fails - var protams būt vairāki\n",
    "|   └── script.js         # Jūsu JavaScript fails - var protams būt vairāki\n",
    "├── templates/            # Mape šabloniem\n",
    "│   └── base.html         # Bāzes šablons - var protams būt vairāki\n",
    "│   └── index.html        # Citi šabloni - var protams būt vairāki\n",
    "└── requirements.txt      # (Ieteicams bet ne obligāts) saraksts ar visām nepieciešamajām bibliotēkām\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mācību materiāli un resursi\tFlask apguvei\n",
    "\n",
    "*Lai būtu pilnvērtīgs Flask izstrādātājs, jums būs nepieciešams iemācīties vairāk par šādām tēmām:*\t\n",
    "\n",
    "- Formu apstrāde ar Flask\n",
    "- Datu bāzu izmantošana ar Flask, SQLAlchemy vai citiem ORM - Object-Relational Mapping rīkiem\n",
    "- Lietotāja autentifikācija ar Flask\n",
    "- Sesiju pārvaldība ar Flask\n",
    "- Flask lietotņu izvietošana - AWS, PythonAnywhere, DigitalOcean, lokālais serveris utt.\n",
    "\n",
    "Lai turpinātu apgūt Flask, jūs varat izmantot šādus resursus:\n",
    "- Oficiālā Flask dokumentācija: https://flask.palletsprojects.com/en/2.0.x/\n",
    "- Miguel Grinberg's Flask Mega-Tutorial: https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world\n",
    "- Corey Schafer's Flask Tutorial Series: https://www.youtube.com/playlist?list=PL-osiE80TeTs4UjLw5MM6OjgkjFeUxCYH\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tīmekļa lapu rasmošana - skrāpēšana - kas tas ir?\n",
    "\n",
    "Tīmekļa lapu rasmošana ir datu iegūšanas process no tīmekļa lapām. Tas ietver HTTP pieprasījumu nosūtīšanu uz tīmekļa lapu, HTML satura parsēšanu un nepieciešamo datu iegūšanu. Tīmekļa lapu rasmošana tiek izmantota datu iegūšanai, cenu uzraudzībai un satura apkopošanai un citiem mērķiem.\n",
    "\n",
    "Teorētiski iespējams rasmošanu veikt arī manuāli, vienkārši saglabājot apmeklētās tīmekļa lapas saturu, bet bieži vien ir efektīvāk izmantot tīmekļa rasmošanas bibliotēku, piemēram, Scrapy, lai automatizētu procesu.\n",
    "\n",
    "### Rasmošanas noteikumi\n",
    "\n",
    "* Spēlējiet tīri! Neveidojiet pārlieku slodzi tīmekļa vietnei ar pieprasījumiem, jo tas var izraisīt servera problēmas un jūs varat tikt nobloķēts.\n",
    "* Pirms sākat rasmošanu, pārbaudiet tīmekļa vietnes lietošanas noteikumus un robots.txt failu, lai pārliecinātos, ka neaizskarāt kādus noteikumus.\n",
    "* Ja iespējams iegūt datus caur API, ir ieteicams izmantot API, nevis rasmošanu.\n",
    "* Iegūtās datus izmantojiet tikai saskaņā ar autortiesībām un likumiem.\n",
    "* Ja iespējams, izmantojiet jau publisku datukopu - vislabāk no pašiem lietotnes autoriem, nevis tīmekļa lapas rasmošanu.\n",
    "* Izmantojiet tīmekļa lapu rasmošanu atbildīgi un etiski.\n",
    "* Ievērojiet atšķirību starp kādu datu rasmošanu un to publicēšanu. Rasmošana ir tikai datu iegūšana, bet publicēšana ir atsevišķs jautājums.\n",
    "* Pētnieciskiem mērķiem ir ieteicams iegūt atļauju no tīmekļa vietnes īpašniekiem, ja iespējams.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy - A Python Web Scraping Library\n",
    "\n",
    "Scrapy is a powerful web scraping library for Python that makes it easy to extract data from websites. It provides a high-level API for crawling websites and extracting data, making it a great choice for web scraping projects.\n",
    "\n",
    "### How Scrapy Works\n",
    "\n",
    "Scrapy works by sending HTTP requests to a website, parsing the HTML content, and extracting the data you need. It provides a set of tools and libraries for building web scrapers, including a built-in web crawler, a powerful selector system, and support for handling cookies and sessions.\n",
    "\n",
    "### Installing Scrapy\n",
    "\n",
    "As usual it is best to create and activate a virtual environment before installing Scrapy. \n",
    "\n",
    "You can install Scrapy using `pip`:\n",
    "\n",
    "```bash\n",
    "pip install Scrapy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple web scraping example with Scrapy\n",
    "\n",
    "Let's say we want to scrape the cities and their populations from wikipedia page: https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\n",
    "(Note that Wikipedia offers an API for accessing its data, so scraping is not necessary in this case. This is just an example.)\n",
    "\n",
    "Here is a simple example of how to scrape data from a website using Scrapy:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy version is 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# first let's import scrapy and check its version\n",
    "try:\n",
    "    import scrapy\n",
    "    print(f\"Scrapy version is {scrapy.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Scrapy is not installed\")\n",
    "    print(\"You can install Scrapy with pip install scrapy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic scraping example with Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 19:38:03 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): en.wikipedia.org:443\n",
      "2024-12-09 19:38:03 [urllib3.connectionpool] DEBUG: https://en.wikipedia.org:443 \"GET /wiki/List_of_cities_in_Latvia HTTP/11\" 200 38490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will scrape https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\n",
      "Web page title is List of cities and towns in Latvia - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "# now let's scrape the following web page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_cities_in_Latvia'\n",
    "print(f\"We will scrape {url}\")\n",
    "# Note: usually there is no need to scrape Wikipedia as they have APIs and data dumps available\n",
    "# but for learning purposes we will scrape this page\n",
    "# we are interested in table with cities and their population\n",
    "\n",
    "# we've already imported scrapy so we can start using it\n",
    "# let's start with basic example where we simply fetch the page and extract the title\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "try:\n",
    "    import requests  # we also use requests library to fetch the page\n",
    "except ImportError:\n",
    "    print(\"Requests is not installed\")\n",
    "    print(\"You can install Requests with pip install requests\")\n",
    "\n",
    "# NOTE: Scrapy has its own request object as well, but it is more complex\n",
    "\n",
    "response = requests.get(url)\n",
    "# let's create a scrapy response object\n",
    "scrapy_response = HtmlResponse(url, body=response.text, encoding='utf-8')\n",
    "\n",
    "# let's extract the title using css selector\n",
    "title = scrapy_response.css('title::text').get()\n",
    "print(\"Web page title is\", title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CSS selectors to extract data\n",
    "\n",
    "Scrapy provides a powerful selector system that allows you to extract data from HTML using CSS selectors. You can use CSS selectors to target specific elements on a page and extract the data you need.\n",
    "\n",
    " **CSS Selectors** CSS Selectors allow for simpler and often more intuitive queries using CSS rules. Scrapy's `response.css()` method provides an interface for selecting elements based on CSS selectors.\n",
    " \n",
    "#### Common CSS Examples: \n",
    "| CSS Selector | Description | \n",
    "| --- | --- | \n",
    "| tag | Selects all <tag> elements. | \n",
    "| .class | Selects all elements with the class class. | \n",
    "| #id | Selects the element with the ID id. | \n",
    "| tag.class | Selects <tag> elements with the class class. | \n",
    "| tag[attr=\"value\"] | Selects <tag> elements with an attribute attr=\"value\". | \n",
    "| tag > child | Selects direct children of <tag>. | \n",
    "| tag child | Selects all descendants of <tag>. | \n",
    "| tag:first-child | Selects the first child <tag> of its parent. | \n",
    "| tag:nth-child(n) | Selects the nth <tag> child. | \n",
    "\n",
    "#### Scrapy CSS Methods: \n",
    " \n",
    "- `response.css('<CSS selector>')`: Extracts elements matching the CSS selector.\n",
    " \n",
    "- `.get()`: Returns the first matching value.\n",
    " \n",
    "- `.getall()`: Returns all matching values as a list.\n",
    " \n",
    "- `.re('<regex>')`: Extracts values matching a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 tables on the page\n",
      "There are 2 wikitable sortable tables on the page\n",
      "Table(s) have rows 83\n"
     ]
    }
   ],
   "source": [
    "# now let's use Selector to get the table with cities and their population\n",
    "# we will use CSS selectors\n",
    "# we will use the table with class wikitable sortable\n",
    "\n",
    "# first let's see about getting all tables\n",
    "tables = scrapy_response.css('table')\n",
    "# how many tables are there?\n",
    "print(\"There are\", len(tables), \"tables on the page\")\n",
    "\n",
    "# by using inspect element in browser we can see that the table we want is the first table with class wikitable sortable\n",
    "table = scrapy_response.css('table.wikitable.sortable') \n",
    "# how many tables are there?\n",
    "print(\"There are\", len(table), \"wikitable sortable tables on the page\")\n",
    "# in our case this is fine as we want elements from both tables\n",
    "# if we only needed the first table we could use table = scrapy_response.css('table.wikitable.sortable')[0] as the first table is at index 0\n",
    "\n",
    "# how many tr elements are in the tables\n",
    "rows = table.css('tr')\n",
    "print(\"Table(s) have rows\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest five cities [{'city': ['Rīga', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '658,640\\n'}, {'city': ['Daugavpils', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '93,312\\n'}, {'city': ['Liepāja', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '76,731\\n'}, {'city': ['Jelgava', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '59,511\\n'}, {'city': ['Jūrmala', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '50,840\\n'}]\n",
      "Smallest five cities [{'city': ['Varakļāni', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '2,106\\n'}, {'city': ['Viesīte', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '1,902\\n'}, {'city': ['Viļaka', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '1,525\\n'}, {'city': ['Viļāni', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '3,468\\n'}, {'city': ['Zilupe', '\\xa0', 'pronunciation', 'ⓘ', '\\xa0', '\\n'], 'population': '1,746\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# let's save data in a list of dictionaries for easier processing\n",
    "cities = []\n",
    "# let's iterate over rows and extract the data\n",
    "for row in rows:\n",
    "    # city = row.css('td:nth-child(1)::text').get()\n",
    "    # we want all text from first child td element that is contained in its children\n",
    "    city = row.css('td:nth-child(1) *::text').getall() # note the * after td:nth-child(1) we get all text from all children\n",
    "    population = row.css('td:nth-child(2)::text').get() # here we get only the text from the td element\n",
    "    if city and population: # we only want rows with city and population\n",
    "        cities.append({'city': city, 'population': population})\n",
    "# print first 5 cities\n",
    "print(\"Biggest five cities\", cities[:5])\n",
    "# last 5 cities\n",
    "print(\"Smallest five cities\", cities[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest five cities [{'city': 'Rīga', 'population': '658,640\\n'}, {'city': 'Daugavpils', 'population': '93,312\\n'}, {'city': 'Liepāja', 'population': '76,731\\n'}, {'city': 'Jelgava', 'population': '59,511\\n'}, {'city': 'Jūrmala', 'population': '50,840\\n'}]\n",
      "Smallest five cities [{'city': 'Varakļāni', 'population': '2,106\\n'}, {'city': 'Viesīte', 'population': '1,902\\n'}, {'city': 'Viļaka', 'population': '1,525\\n'}, {'city': 'Viļāni', 'population': '3,468\\n'}, {'city': 'Zilupe', 'population': '1,746\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# we can see that we only care about the first entry for city name so let's clean up the list of dictionaries\n",
    "# we will keep only the first entry for city name\n",
    "cities = [{'city': d['city'][0], 'population': d['population']} for d in cities] # list comprehension\n",
    "# print first 5 cities\n",
    "print(\"Biggest five cities\", cities[:5])\n",
    "# last 5 cities\n",
    "print(\"Smallest five cities\", cities[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we could save the results to a file or database\n",
    "# alternatively we could load the data into a pandas DataFrame for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using XPath selectors to extract data\n",
    "\n",
    "XPath is more powerful than CSS selectors, but it is also more complex. \n",
    "\n",
    "**XPath Selectors** XPath (XML Path Language) allows you to navigate and query nodes in an HTML or XML document. Scrapy's `response.xpath()` method provides an interface for selecting elements based on XPath expressions.\n",
    "\n",
    "#### Common XPath Examples: \n",
    "| XPath Expression | Description | \n",
    "| --- | --- | \n",
    "| //tag | Selects all <tag> elements anywhere in the document. | \n",
    "| ./tag | Selects all <tag> elements directly under the current node. | \n",
    "| //tag[@attr=\"value\"] | Selects <tag> elements with an attribute attr=\"value\". | \n",
    "| //tag/text() | Selects the text content of <tag> elements. | \n",
    "| //tag[contains(@attr, \"val\")] | Selects <tag> elements where attr contains \"val\". | \n",
    "| //tag[1] | Selects the first <tag> element in the context. | \n",
    "| //tag[last()] | Selects the last <tag> element in the context. | \n",
    "| //tag[position() < 3] | Selects the first two <tag> elements. | \n",
    "\n",
    "#### Scrapy XPath Methods: \n",
    " \n",
    "- `response.xpath('<XPath>')`: Extracts elements matching the XPath.\n",
    " \n",
    "- `.get()`: Returns the first matching value.\n",
    " \n",
    "- `.getall()`: Returns all matching values as a list.\n",
    " \n",
    "- `.extract()`: Deprecated alias for `.getall()`.\n",
    " \n",
    "- `.re('<regex>')`: Extracts values matching a regular expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 wikitable sortable tables on the page\n"
     ]
    }
   ],
   "source": [
    "# now let's see how we could have used XPath selectors to get the same data\n",
    "\n",
    "# let's extract the table with XPath\n",
    "table = scrapy_response.xpath('//table[contains(@class, \"wikitable\") and contains(@class, \"sortable\")]')\n",
    "# how many tables are there?\n",
    "print(\"There are\", len(table), \"wikitable sortable tables on the page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table(s) have rows 83\n"
     ]
    }
   ],
   "source": [
    "# now let's extract the rows with XPath\n",
    "rows = table.xpath('.//tr')\n",
    "print(\"Table(s) have rows\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try something slightly fancier we will want to extract text from first anchor child in first td element - city name\n",
    "# example\n",
    "# <tr>\n",
    "# <td><a href=\"/wiki/R%C4%ABga\" class=\"mw-redirect\" title=\"Rīga\">Rīga</a>&nbsp;<small><span class=\"noprint\"><span class=\"ext-phonos\"><span data-nosnippet=\"\" id=\"ooui-php-1\" class=\"ext-phonos-PhonosButton noexcerpt oo-ui-widget oo-ui-widget-enabled oo-ui-buttonElement oo-ui-buttonElement-frameless oo-ui-iconElement oo-ui-labelElement oo-ui-buttonWidget\" data-ooui=\"{&quot;_&quot;:&quot;mw.Phonos.PhonosButton&quot;,&quot;href&quot;:&quot;\\/\\/upload.wikimedia.org\\/wikipedia\\/commons\\/transcoded\\/f\\/f5\\/Lv-R%C4%ABga.ogg\\/Lv-R%C4%ABga.ogg.mp3&quot;,&quot;rel&quot;:[&quot;nofollow&quot;],&quot;framed&quot;:false,&quot;icon&quot;:&quot;volumeUp&quot;,&quot;label&quot;:{&quot;html&quot;:&quot;pronunciation&quot;},&quot;data&quot;:{&quot;ipa&quot;:&quot;&quot;,&quot;text&quot;:&quot;&quot;,&quot;lang&quot;:&quot;en&quot;,&quot;wikibase&quot;:&quot;&quot;,&quot;file&quot;:&quot;Lv-R\\u012bga.ogg&quot;},&quot;classes&quot;:[&quot;ext-phonos-PhonosButton&quot;,&quot;noexcerpt&quot;]}\"><a role=\"button\" tabindex=\"0\" href=\"//upload.wikimedia.org/wikipedia/commons/transcoded/f/f5/Lv-R%C4%ABga.ogg/Lv-R%C4%ABga.ogg.mp3\" rel=\"nofollow\" aria-label=\"Play audio\" title=\"Play audio\" class=\"oo-ui-buttonElement-button\"><span class=\"oo-ui-iconElement-icon oo-ui-icon-volumeUp\"></span><span class=\"oo-ui-labelElement-label\">pronunciation</span><span class=\"oo-ui-indicatorElement-indicator oo-ui-indicatorElement-noIndicator\"></span></a></span><sup class=\"ext-phonos-attribution noexcerpt navigation-not-searchable\"><a href=\"/wiki/File:Lv-R%C4%ABga.ogg\" title=\"File:Lv-Rīga.ogg\">ⓘ</a></sup></span></span></small>&nbsp;<figure class=\"mw-halign-right\" typeof=\"mw:File\"><a href=\"/wiki/File:Greater_Coat_of_Arms_of_Riga_-_for_display.svg\" class=\"mw-file-description\"><img src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Greater_Coat_of_Arms_of_Riga_-_for_display.svg/55px-Greater_Coat_of_Arms_of_Riga_-_for_display.svg.png\" decoding=\"async\" width=\"55\" height=\"33\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Greater_Coat_of_Arms_of_Riga_-_for_display.svg/83px-Greater_Coat_of_Arms_of_Riga_-_for_display.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/99/Greater_Coat_of_Arms_of_Riga_-_for_display.svg/110px-Greater_Coat_of_Arms_of_Riga_-_for_display.svg.png 2x\" data-file-width=\"510\" data-file-height=\"303\"></a><figcaption></figcaption></figure>\n",
    "# </td>\n",
    "# <td>658,640\n",
    "# </td>\n",
    "# <td>632,614\n",
    "# </td>\n",
    "# <td>605,273\n",
    "# </td></tr>\n",
    "\n",
    "# here we can see that city name Rīga is in the first td element inside its first anchor child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest five cities [{'city': 'Rīga', 'population': '658,640\\n'}, {'city': 'Daugavpils', 'population': '93,312\\n'}, {'city': 'Liepāja', 'population': '76,731\\n'}, {'city': 'Jelgava', 'population': '59,511\\n'}, {'city': 'Jūrmala', 'population': '50,840\\n'}]\n",
      "Smallest five cities [{'city': 'Varakļāni', 'population': '2,106\\n'}, {'city': 'Viesīte', 'population': '1,902\\n'}, {'city': 'Viļaka', 'population': '1,525\\n'}, {'city': 'Viļāni', 'population': '3,468\\n'}, {'city': 'Zilupe', 'population': '1,746\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# now let's save data in a list of dictionaries for easier processing\n",
    "also_cities = []\n",
    "# let's iterate over rows and extract the data\n",
    "for row in rows:\n",
    "    # city = row.xpath('.//td[1]//text()').getall() # we want all text from first child td element that is contained in its children\n",
    "    # we want text from first anchor element in td[1]\n",
    "    city = row.xpath('.//td[1]//a[1]//text()').get() # note the //a[1] we get text from first anchor child of the first td element\n",
    "    population = row.xpath('.//td[2]//text()').get() # here we get only the text from the td element\n",
    "    if city and population: # we only want rows with city and population\n",
    "        also_cities.append({'city': city, 'population': population})\n",
    "# print first 5 cities\n",
    "print(\"Biggest five cities\", also_cities[:5])\n",
    "# last 5 cities\n",
    "print(\"Smallest five cities\", also_cities[-5:])\n",
    "\n",
    "# using XPath we did not have to do any Python list comprehension to clean up the data after extraction\n",
    "# now we could still clean up newlines and convert population to integer but that is easy and not part of scraping itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS vs XPath key differences in Scrapy\n",
    "\n",
    "| Feature | XPath | CSS | \n",
    "| --- | --- | --- | \n",
    "| Syntax Complexity | More expressive and versatile | Simpler, easier to write | \n",
    "| Attribute Matching | @attr | [attr=\"value\"] | \n",
    "| Text Content | .//text() | ::text | \n",
    "| Positional Matching | [position()=n] | :nth-child(n) | \n",
    "| Advanced Navigation | Supports parent/sibling navigation | Limited to child navigation | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping multiple pages with Scrapy\n",
    "\n",
    "For scraping multiple pages, you can use Scrapy's built-in web crawler to follow links and scrape data from multiple pages. You can define rules to follow links and extract data from each page.\n",
    "\n",
    "For now we will use preset list of URLs to scrape, but in real life you would probably want to scrape links from the page itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will scrape the following pages:\n",
      "https://en.wikipedia.org/wiki/List_of_cities_in_Estonia\n",
      "https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\n",
      "https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania\n"
     ]
    }
   ],
   "source": [
    "start_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Estonia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania\",\n",
    "]\n",
    "print(\"We will scrape the following pages:\", *start_urls, sep=\"\\n\") # we using * to unpack the list into separate arguments for neat printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:14:36 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: scrapybot)\n",
      "2024-12-09 20:14:36 [scrapy.utils.log] INFO: Versions: lxml 5.2.2.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.11.0, Python 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)], pyOpenSSL 24.3.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Windows-11-10.0.22631-SP0\n",
      "2024-12-09 20:14:36 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-12-09 20:14:36 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-12-09 20:14:36 [scrapy.extensions.telnet] INFO: Telnet Password: 230407da82ccaf53\n",
      "2024-12-09 20:14:36 [py.warnings] WARNING: c:\\pyenvs\\venv312\\Lib\\site-packages\\scrapy\\extensions\\feedexport.py:432: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n",
      "2024-12-09 20:14:36 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-12-09 20:14:36 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-12-09 20:14:37 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-12-09 20:14:37 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-12-09 20:14:37 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-12-09 20:14:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-12-09 20:14:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_cities_in_Estonia> (referer: None)\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania> (referer: None)\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_cities_in_Latvia> (referer: None)\n",
      "2024-12-09 20:14:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/List_of_cities_in_Estonia>\n",
      "{'title': 'List of cities and towns in Estonia - Wikipedia'}\n",
      "2024-12-09 20:14:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania>\n",
      "{'title': 'List of cities in Lithuania - Wikipedia'}\n",
      "2024-12-09 20:14:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/List_of_cities_in_Latvia>\n",
      "{'title': 'List of cities and towns in Latvia - Wikipedia'}\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-12-09 20:14:37 [scrapy.extensions.feedexport] INFO: Stored json feed (3 items) in: cities_titles.json\n",
      "2024-12-09 20:14:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 760,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 119110,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 3,\n",
      " 'elapsed_time_seconds': 0.306788,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 12, 9, 18, 14, 37, 551379, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 819985,\n",
      " 'httpcompression/response_count': 3,\n",
      " 'item_scraped_count': 3,\n",
      " 'items_per_minute': None,\n",
      " 'log_count/DEBUG': 7,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 3,\n",
      " 'responses_per_minute': None,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2024, 12, 9, 18, 14, 37, 244591, tzinfo=datetime.timezone.utc)}\n",
      "2024-12-09 20:14:37 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# we have the start_urls now we can create a Scrapy spider to scrape these pages\n",
    "# in this case we will use a simple spider that will only extract the title of the page\n",
    "\n",
    "# let's do a simple spider that will extract the title of the page\n",
    "\n",
    "# we want to save these titles in a list of dictionaries as JSON\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class SimpleSpider(scrapy.Spider):\n",
    "    name = 'simple_spider'\n",
    "    start_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Estonia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Latvia\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_cities_in_Lithuania\",\n",
    "] # we could have passed this as a parameter to the spider\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').get()\n",
    "        # ADD your own code here to extract more data from each page\n",
    "        # you can use either css or xpath selectors\n",
    "        yield {'title': title}\n",
    "\n",
    "# let's run the spider\n",
    "# we also want to save the results to a file\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    'FEED_URI': 'cities_titles.json',\n",
    "    'FEED_FORMAT': 'json'\n",
    "})\n",
    "\n",
    "process.crawl(SimpleSpider)\n",
    "process.start()\n",
    "# NOTE this process is really meant to be run from a script or terminal\n",
    "# if you run it in a Jupyter notebook you might have to restart the kernel to run it again\n",
    "# otherwise you will get errors about already running Twisted reactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi page scraper overview\n",
    "\n",
    " \n",
    "1. **Create the Spider** : \n",
    "  - Include all URLs in `start_urls` or load them dynamically using `start_requests()`.\n",
    " \n",
    "2. **Parse Each Page** : \n",
    "  - Use the same `parse()` method for extracting data since the structure of the pages is similar.\n",
    " \n",
    "3. **Identify the Context** :\n",
    "  - Use information from the URL or page content to tag the data (e.g., country name).\n",
    " \n",
    "4. **Run the Spider** :\n",
    "  - Execute the spider and save the output to a file.\n",
    " \n",
    "5. **Post-Processing** :\n",
    "  - Optionally, combine or clean the scraped data further (e.g., merge JSON files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy Learning References\n",
    "\n",
    "For further exploration of Scrapy, you can refer to the following resources:\n",
    "\n",
    "- Official Scrapy Documentation: https://docs.scrapy.org/en/latest/\n",
    "- Scrapy Tutorial: https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    "- YouTube Tutorial: https://www.youtube.com/watch?v=ve_0h4Y8nuI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "### Flask Practice\n",
    "\n",
    "1. Create a simple web application using Flask that displays a list of items. The list should be stored in a Python list and displayed on the web page.\n",
    "2. Add a form to the web application that allows users to add new items to the list.\n",
    "3. Add a delete button next to each item in the list that allows users to delete items from the list.\n",
    "\n",
    "### Scrapy Practice\n",
    "\n",
    "1. Create a Scrapy spider that scrapes data from a website of your choice. The spider should extract at least two fields from the website and save the data to a JSON or CSV file.\n",
    "2. Modify the spider to save the data to a database instead of a JSON or CSV file.\n",
    "3. Add error handling to the spider to handle cases where the website is down or the data is missing.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, we covered the basics of server-side web development using Flask and web scraping using Scrapy. We learned how to create a simple web application with Flask and how to scrape data from a website using Scrapy. We also discussed the rules of web scraping and best practices for working with web scraping libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
